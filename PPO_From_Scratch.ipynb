{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSUx1fG53gAs53oAEUeM7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR-QA8eyl9_Z",
        "outputId": "2d58f8b0-037b-4490-fc19-f3f4c7fcf8ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "FdjPZTbIRfDK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VBNaB4Ng0tis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "d6272796-b53b-440e-b92b-2f604f1b9409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "<numpy.random._pcg64.PCG64 object at 0x7c09051c94e0> is not a known BitGenerator module.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3492982500.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;31m# initilizattion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncVectorEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;31m# Get observation and action space dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/vector/async_vector_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns, observation_space, action_space, shared_memory, copy, context, daemon, worker, new_step_api)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mdummy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mdummy_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mobservation_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/vector/vector_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_envs, observation_space, action_space, new_step_api)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_vector_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    907\u001b[0m                             '1 positional argument')\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/vector/utils/spaces.py\u001b[0m in \u001b[0;36m_batch_space_box\u001b[0;34m(space, n)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mrepeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/utils/seeding.py\u001b[0m in \u001b[0;36m_generator_ctor\u001b[0;34m(bit_generator_name)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mbit_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBitGenerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbit_generator_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;34mf\"{bit_generator_name} is not a known BitGenerator module.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: <numpy.random._pcg64.PCG64 object at 0x7c09051c94e0> is not a known BitGenerator module."
          ]
        }
      ],
      "source": [
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"CartPole-v1\" # going to use \"CartPole-v1\"\n",
        "clipping_coef = 0.1\n",
        "num_envs = 4\n",
        "rollouts = 1e4\n",
        "middle_layer_size = 64 # Define the size of the middle layer\n",
        "batch_size = 256 # Define a batch size for reshaping\n",
        "num_epochs = 4\n",
        "num_mini_batches = 4\n",
        "clipping_coef = 0.2\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "# Conventional Vectoriz3ed Environment wrapper\n",
        "def make_env(env_id):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, observation_space_shape, action_space_size, middle_layer_size) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      # Actor/Policy\n",
        "      self.actor = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, action_space_size),\n",
        "          nn.Softmax(dim=-1)\n",
        "          ) # Added dim=-1 to softmax\n",
        "\n",
        "      # Critic/Advantage NN //might need another activation function at the end.\n",
        "      self.critic = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, 1), # Output size of 1 for the value function\n",
        "          )\n",
        "\n",
        "  def predict(self, x):\n",
        "    action_probs = self.actor(x)\n",
        "    act_dist = distributions.Categorical(action_probs)\n",
        "    action = act_dist.sample()\n",
        "    log_prob = act_dist.log_prob(action)\n",
        "    entropy = act_dist.entropy() # Calculate entropy\n",
        "\n",
        "    value_logits = self.critic(x)\n",
        "\n",
        "    # return entropy, probabilies, and sampled action\n",
        "    return (entropy, log_prob, action, value_logits) # Return entropy, probabilities, and a sampled action\n",
        "\n",
        "if __name__ == \"__main__\": # Corrected __main__\n",
        "\n",
        "  # initilizattion\n",
        "  envs = gym.vector.AsyncVectorEnv([make_env(env_id) for i in range(num_envs)])\n",
        "\n",
        "  # Get observation and action space dimensions\n",
        "  observation_space_shape = envs.single_observation_space.shape[0] # Assuming flat observation space\n",
        "  action_space_size = envs.single_action_space.n # Assuming discrete action space\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #initialize the Agent\n",
        "  agent = Agent(observation_space_shape, action_space_size, middle_layer_size).to(device) # Pass dimensions and move to device\n",
        "\n",
        "  #initialize the optimizer\n",
        "  optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
        "\n",
        "  # Initialize tensors with appropriate shapes\n",
        "  obs = torch.zeros((batch_size, num_envs, observation_space_shape)).to(device)\n",
        "  actions = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  logprobs = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  rewards = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  dones = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  pred_values = torch.zeros((batch_size, num_envs)).to(device)\n",
        "\n",
        "  # init actual values and advantages tensor\n",
        "  actual_values = torch.zeros_like(rewards).to(device)\n",
        "  advantages = torch.zeros_like(rewards).to(device)\n",
        "\n",
        "\n",
        "  # initializes the observation, done, the time, and the step\n",
        "  start_time = time.time()\n",
        "  global_step = 0\n",
        "\n",
        "  # define training regime\n",
        "  for i in range(int(rollouts)): # Cast steps to int\n",
        "    step = 0 # Initialize step counter for batch\n",
        "    next_obs = torch.Tensor(envs.reset()[0]).to(device) # Corrected envs.reset()\n",
        "    next_done =  torch.zeros((num_envs,)).to(device)\n",
        "\n",
        "    for step in range(int(batch_size)):\n",
        "      # get actions, observations, rewards, and dones\n",
        "      with torch.no_grad(): # Added no_grad for inference\n",
        "          _, log_prob, action, values_ = agent.predict(next_obs) # Renamed values to values_ to avoid conflict\n",
        "\n",
        "      # Move data to tensors\n",
        "      next_obs_np, rewards_np, next_done_np, infos =  envs.step(action.cpu().numpy()) # env step and move action to cpu\n",
        "      next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "      next_done = torch.Tensor(next_done_np).to(device)\n",
        "\n",
        "      # Store data in tensors at the current step\n",
        "      obs[step] = next_obs\n",
        "      actions[step] = action\n",
        "      logprobs[step] = log_prob\n",
        "      rewards[step] = torch.Tensor(rewards_np).to(device) # Store rewards as tensor\n",
        "      dones[step] = next_done\n",
        "      pred_values[step] = pred_values.squeeze(-1) # Remove the last dimension of size 1\n",
        "\n",
        "      global_step += num_envs # Update global step\n",
        "\n",
        "    # calculate actual values at each time step'\n",
        "    print(\"mean reward: \", rewards.mean().cpu())\n",
        "\n",
        "    for t in reversed(range(batch_size)):\n",
        "      if t == batch_size - 1:\n",
        "          # For the last step, if the environment is not done, use the value of the next state (from the agent's prediction)\n",
        "          # Otherwise, the actual value is just the reward at this step\n",
        "          nextnonterminal = 1.0 - next_done\n",
        "          next_value = agent.critic(next_obs).squeeze(-1) # bootstrap next value since it doesn't exsist\n",
        "      else:\n",
        "          # For other steps, if the environment at the next step is not done, use the value of the next state from the stored values\n",
        "          # Otherwise, the actual value is just the reward at this step\n",
        "          nextnonterminal = 1.0 - dones[t+1]\n",
        "          next_value = actual_values[t+1]\n",
        "      actual_values[t] = rewards[t] + gamma * next_value * nextnonterminal\n",
        "    advantages = actual_values - pred_values\n",
        "\n",
        "    # Actually training the agent neural net\n",
        "\n",
        "    # flattening the tensors for ease\n",
        "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "    b_logprobs = logprobs.reshape(-1)\n",
        "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "    b_advantages = advantages.reshape(-1)\n",
        "    b_actual_values = actual_values.reshape(-1)\n",
        "    b_pred_values = pred_values.reshape(-1)\n",
        "\n",
        "    # creates storage to see loss over time\n",
        "    losses = []\n",
        "\n",
        "    # Iterates over the same batch a couple times for efficiency\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      #seperates into minibatches\n",
        "      indices = np.arange(batch_size)   # creates indicies\n",
        "      np.random.shuffle(indices)        # shuffles indicies\n",
        "      minibatch_indices = np.array_split(indices, num_mini_batches)\n",
        "\n",
        "      #iterates over the minibatches\n",
        "      for mini_batch_index in range(minibatch_indices.shape()):\n",
        "\n",
        "        # init mini_batch\n",
        "        current_minibatch_indices = minibatch_indices[mini_batch_index]\n",
        "        mb_obs = b_obs[current_minibatch_indices]\n",
        "        mb_log_probs = b_logprobs[current_minibatch_indices]\n",
        "        mb_actions = b_actions[current_minibatch_indices]\n",
        "        mb_advantages = b_advantages[current_minibatch_indices]\n",
        "        mb_actual_values = b_actual_values[current_minibatch_indices]\n",
        "        mb_pred_values = b_pred_values[current_minibatch_indices]\n",
        "\n",
        "        # get new logprobs(but don't overwrite), values, and entropy\n",
        "        mb_new_entropy, mb_new_log_probs, _, mb_new_values_ = agent.predict(mb_obs) # note: may need to be flattened\n",
        "\n",
        "        # value optimization\n",
        "        unclipped_value_loss = (mb_actual_values - mb_new_values_ ) ** 2\n",
        "\n",
        "        clipped_predicted_values = mb_pred_values + torch.clamp(mb_new_values_- mb_pred_values, -clipping_coef, clipping_coef)\n",
        "        clipped_value_Loss = (mb_actual_values - clipped_predicted_values ) ** 2\n",
        "\n",
        "        value_loss = torch.max(unclipped_value_loss, clipped_value_Loss).mean()\n",
        "\n",
        "        # policy optimization\n",
        "\n",
        "        # Normalize advantages\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8) # Add a small epsilon for numerical stability\n",
        "\n",
        "        # calculate ratios\n",
        "        unclipped_ratio = (mb_new_log_probs - mb_log_probs).exp()\n",
        "        clipped_ratio = torch.clamp(unclipped_ratio, 1 - clipping_coef, 1 + clipping_coef)\n",
        "\n",
        "        # calculate loss\n",
        "        policy_loss = torch.max(-mb_advantages*unclipped_ratio, -mb_advantages*clipped_ratio).mean()\n",
        "\n",
        "        # calculates entropy\n",
        "        entropy_loss = mb_new_entropy.mean()\n",
        "\n",
        "        #calculates total loss\n",
        "\n",
        "        loss = policy_loss - (entropy_loss*0.01) + (value_loss * 0.5)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "    # This part of the code needs to be implemented for the training loop (calculating advantages, updating networks, etc.)\n",
        "    # This is a placeholder and would typically involve:\n",
        "    # 1. Calculating advantages/returns\n",
        "    # 2. Calculating policy and value losses\n",
        "    # 3. Performing backpropagation and optimizer steps\n",
        "\n",
        "  envs.close() # Close the environment"
      ]
    }
  ]
}