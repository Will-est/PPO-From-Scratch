{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9Ey5KROTZuuYp5iOk9myt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import gymnasium as gym\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "FdjPZTbIRfDK",
        "outputId": "8e85b42f-7678-4742-ebfc-1a2d0b87d973"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'stable_baselines3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1305406866.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import statments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBNaB4Ng0tis"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"LunarLander\"\n",
        "clipping_coef = 0.1\n",
        "num_envs = 4\n",
        "\n",
        "# Conventional Vectoriz3ed Environment wrapper\n",
        "def make_env(env_id):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, *args, **kwargs) -> None:\n",
        "      super().__init__(*args, **kwargs):\n",
        "      self.agent = nn.Sequential([])\n",
        "      self.agent = nn.Sequential([])\n",
        "  def predict():\n",
        "\n",
        "\n",
        "if __name__ == \"main\":\n",
        "\n",
        "\n",
        "  # initilize invironment\n",
        "\n",
        "  # put things onto gpu\n",
        "\n",
        "  # define training regime\n",
        "\n",
        "  # outer loop: rollouts, at each iteration of the larger loop do one step for your policy and update your value function make sure to clip both the policy and value function. ALso include an entropy term\n",
        "\n",
        "  # inner loop: train value function, step through environment, record rewards, ends, log probs, etc.\n",
        "\n",
        ""
      ]
    }
  ]
}