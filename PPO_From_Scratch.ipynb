{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPAAXNfIlcjU/LCUGxbixyU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "# !pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "hR-QA8eyl9_Z",
        "outputId": "fc2e1776-732e-42f3-870c-dd60973628a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.24.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.11.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "geopandas 1.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scipy 1.16.0 requires numpy<2.6,>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "a9cc3576c31c4e77a599f32e98cb095b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "FdjPZTbIRfDK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VBNaB4Ng0tis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb6ca33-8c0a-4e28-cbea-90bcd07b4573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/vector/vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:  tensor(1024.)\n",
            "Rollout 1 Mean Loss: 28.3982115983963\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 2 Mean Loss: 25.93390393257141\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 3 Mean Loss: 24.86137282848358\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 4 Mean Loss: 24.049135088920593\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 5 Mean Loss: 25.017911195755005\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 6 Mean Loss: 22.477239847183228\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 7 Mean Loss: 23.62862741947174\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 8 Mean Loss: 21.664012551307678\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 9 Mean Loss: 22.824763774871826\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 10 Mean Loss: 21.917076468467712\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 11 Mean Loss: 21.92331075668335\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 12 Mean Loss: 23.62221646308899\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 13 Mean Loss: 22.05792725086212\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 14 Mean Loss: 20.62430965900421\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 15 Mean Loss: 22.044740319252014\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 16 Mean Loss: 19.3333957195282\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 17 Mean Loss: 21.373889446258545\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 18 Mean Loss: 18.970129013061523\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 19 Mean Loss: 16.51414793729782\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 20 Mean Loss: 18.275254130363464\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 21 Mean Loss: 19.193466424942017\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 22 Mean Loss: 17.617661833763123\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 23 Mean Loss: 18.33303964138031\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 24 Mean Loss: 16.83003741502762\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 25 Mean Loss: 18.50210165977478\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 26 Mean Loss: 17.98656141757965\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 27 Mean Loss: 16.46699768304825\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 28 Mean Loss: 17.343262672424316\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 29 Mean Loss: 15.160298347473145\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 30 Mean Loss: 15.267320215702057\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 31 Mean Loss: 15.612856388092041\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 32 Mean Loss: 15.436877071857452\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 33 Mean Loss: 13.153353452682495\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 34 Mean Loss: 14.614764988422394\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 35 Mean Loss: 14.112437427043915\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 36 Mean Loss: 13.016159117221832\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 37 Mean Loss: 12.263351202011108\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 38 Mean Loss: 11.602328836917877\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 39 Mean Loss: 13.589493811130524\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 40 Mean Loss: 11.053706049919128\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 41 Mean Loss: 10.344069421291351\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 42 Mean Loss: 11.581219255924225\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 43 Mean Loss: 12.309533536434174\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 44 Mean Loss: 13.131070792675018\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 45 Mean Loss: 9.212702691555023\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 46 Mean Loss: 9.819093465805054\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 47 Mean Loss: 8.869263410568237\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 48 Mean Loss: 9.017879903316498\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 49 Mean Loss: 8.556655466556549\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 50 Mean Loss: 7.764069139957428\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 51 Mean Loss: 8.55740088224411\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 52 Mean Loss: 7.518739879131317\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 53 Mean Loss: 7.089905202388763\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 54 Mean Loss: 6.627838999032974\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 55 Mean Loss: 6.390261381864548\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 56 Mean Loss: 6.614523619413376\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 57 Mean Loss: 6.4329811334609985\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 58 Mean Loss: 6.147043883800507\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 59 Mean Loss: 6.632597744464874\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 60 Mean Loss: 6.109534054994583\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 61 Mean Loss: 5.481167912483215\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 62 Mean Loss: 5.428467720746994\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 63 Mean Loss: 5.474850237369537\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 64 Mean Loss: 4.707763612270355\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 65 Mean Loss: 4.059234693646431\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 66 Mean Loss: 4.407179355621338\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 67 Mean Loss: 3.4672976285219193\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 68 Mean Loss: 3.2286923825740814\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 69 Mean Loss: 3.8162474930286407\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 70 Mean Loss: 3.5102680325508118\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 71 Mean Loss: 3.258013039827347\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 72 Mean Loss: 3.089961498975754\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 73 Mean Loss: 3.786950409412384\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 74 Mean Loss: 3.109579101204872\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 75 Mean Loss: 3.0015587210655212\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 76 Mean Loss: 2.7379933446645737\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 77 Mean Loss: 3.1588809937238693\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 78 Mean Loss: 2.9024947732686996\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 79 Mean Loss: 2.8317385762929916\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 80 Mean Loss: 2.4979074001312256\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 81 Mean Loss: 2.451645866036415\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 82 Mean Loss: 2.4435847252607346\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 83 Mean Loss: 2.4307158142328262\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 84 Mean Loss: 2.427159182727337\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 85 Mean Loss: 2.4013258069753647\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 86 Mean Loss: 2.6336162835359573\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 87 Mean Loss: 2.3807918578386307\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 88 Mean Loss: 2.526811435818672\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 89 Mean Loss: 2.452806457877159\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 90 Mean Loss: 2.755214899778366\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 91 Mean Loss: 2.2894100546836853\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 92 Mean Loss: 2.570554792881012\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 93 Mean Loss: 2.5407842248678207\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 94 Mean Loss: 2.2547181099653244\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 95 Mean Loss: 2.1131476983428\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 96 Mean Loss: 2.535678043961525\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 97 Mean Loss: 2.6920413076877594\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 98 Mean Loss: 2.4792058765888214\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 99 Mean Loss: 2.4384296983480453\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 100 Mean Loss: 2.669714868068695\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 101 Mean Loss: 2.468565598130226\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 102 Mean Loss: 2.2877046391367912\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 103 Mean Loss: 2.4721586406230927\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 104 Mean Loss: 2.5021631866693497\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 105 Mean Loss: 2.4844530671834946\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 106 Mean Loss: 2.4624537378549576\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 107 Mean Loss: 2.471227928996086\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 108 Mean Loss: 2.433697856962681\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 109 Mean Loss: 2.253919377923012\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 110 Mean Loss: 2.410776272416115\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 111 Mean Loss: 2.5156128257513046\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 112 Mean Loss: 2.503634363412857\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 113 Mean Loss: 2.5430196970701218\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 114 Mean Loss: 2.25806026160717\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 115 Mean Loss: 2.41816408932209\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 116 Mean Loss: 2.4980969429016113\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 117 Mean Loss: 2.35648512840271\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 118 Mean Loss: 2.3595147728919983\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 119 Mean Loss: 2.4342199116945267\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 120 Mean Loss: 2.3209647238254547\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 121 Mean Loss: 2.4155805855989456\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 122 Mean Loss: 2.1193533837795258\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 123 Mean Loss: 2.106787271797657\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 124 Mean Loss: 2.3247776925563812\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 125 Mean Loss: 2.582328513264656\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 126 Mean Loss: 2.2802005857229233\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 127 Mean Loss: 2.397195592522621\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 128 Mean Loss: 2.3529805466532707\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 129 Mean Loss: 2.348550423979759\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 130 Mean Loss: 2.703060671687126\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 131 Mean Loss: 2.0880276411771774\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 132 Mean Loss: 2.607995241880417\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 133 Mean Loss: 2.3638142198324203\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 134 Mean Loss: 2.4829902201890945\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 135 Mean Loss: 2.299967959523201\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 136 Mean Loss: 2.3548691421747208\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 137 Mean Loss: 2.06509118527174\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 138 Mean Loss: 2.4643097519874573\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 139 Mean Loss: 2.5484815388917923\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 140 Mean Loss: 2.267153523862362\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 141 Mean Loss: 2.3258860409259796\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 142 Mean Loss: 2.3394782915711403\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 143 Mean Loss: 2.223966285586357\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 144 Mean Loss: 2.5374697148799896\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 145 Mean Loss: 2.7582142502069473\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 146 Mean Loss: 2.357814744114876\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 147 Mean Loss: 2.20531365275383\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 148 Mean Loss: 2.3632998317480087\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 149 Mean Loss: 2.3115908727049828\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 150 Mean Loss: 2.1477207466959953\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 151 Mean Loss: 2.4825014919042587\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 152 Mean Loss: 2.2500946894288063\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 153 Mean Loss: 2.4564577490091324\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 154 Mean Loss: 2.0593842193484306\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 155 Mean Loss: 2.3141458183526993\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 156 Mean Loss: 2.4258513152599335\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 157 Mean Loss: 2.437176287174225\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 158 Mean Loss: 2.356913283467293\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 159 Mean Loss: 2.291282571852207\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 160 Mean Loss: 2.3096006363630295\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 161 Mean Loss: 2.289215639233589\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 162 Mean Loss: 2.3384037539362907\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 163 Mean Loss: 2.108202889561653\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 164 Mean Loss: 2.105838678777218\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 165 Mean Loss: 2.0944744870066643\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 166 Mean Loss: 2.289396785199642\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 167 Mean Loss: 2.072222724556923\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 168 Mean Loss: 2.2024916633963585\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 169 Mean Loss: 2.6025888323783875\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 170 Mean Loss: 2.336534783244133\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 171 Mean Loss: 2.246745452284813\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 172 Mean Loss: 2.3313717395067215\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 173 Mean Loss: 2.4961704462766647\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 174 Mean Loss: 2.391882434487343\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 175 Mean Loss: 2.4562651962041855\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 176 Mean Loss: 2.315435439348221\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 177 Mean Loss: 2.138686381280422\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 178 Mean Loss: 2.339525617659092\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 179 Mean Loss: 2.2863152474164963\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 180 Mean Loss: 2.2297666370868683\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 181 Mean Loss: 1.9783532097935677\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 182 Mean Loss: 2.2776222601532936\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 183 Mean Loss: 2.1462032943964005\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 184 Mean Loss: 2.207736760377884\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 185 Mean Loss: 2.2365434616804123\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 186 Mean Loss: 2.2148390263319016\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 187 Mean Loss: 2.2578762769699097\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 188 Mean Loss: 2.2930757850408554\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 189 Mean Loss: 2.359586961567402\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 190 Mean Loss: 1.992567390203476\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 191 Mean Loss: 2.4036180675029755\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 192 Mean Loss: 2.2902657464146614\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 193 Mean Loss: 2.30594639480114\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 194 Mean Loss: 2.1701092943549156\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 195 Mean Loss: 2.2845297902822495\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 196 Mean Loss: 2.222266308963299\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 197 Mean Loss: 2.308478184044361\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 198 Mean Loss: 2.335614301264286\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 199 Mean Loss: 2.171712249517441\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 200 Mean Loss: 2.2790999487042427\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 201 Mean Loss: 2.0576133877038956\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 202 Mean Loss: 2.2334898561239243\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 203 Mean Loss: 2.3261071294546127\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 204 Mean Loss: 2.2250752598047256\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 205 Mean Loss: 2.178212307393551\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 206 Mean Loss: 2.394191637635231\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 207 Mean Loss: 2.165613315999508\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 208 Mean Loss: 2.3932479918003082\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 209 Mean Loss: 2.2158059030771255\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 210 Mean Loss: 2.092099003493786\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 211 Mean Loss: 1.9865488335490227\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 212 Mean Loss: 2.186414547264576\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 213 Mean Loss: 2.0470068603754044\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 214 Mean Loss: 2.3461382314562798\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 215 Mean Loss: 2.0347436293959618\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 216 Mean Loss: 2.4192797541618347\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 217 Mean Loss: 2.3519924581050873\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 218 Mean Loss: 1.9771066308021545\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 219 Mean Loss: 2.0971325263381004\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 220 Mean Loss: 2.036145232617855\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 221 Mean Loss: 2.4542592614889145\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 222 Mean Loss: 2.271664433181286\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 223 Mean Loss: 1.9964322447776794\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 224 Mean Loss: 2.047635793685913\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 225 Mean Loss: 2.331079050898552\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 226 Mean Loss: 2.236197419464588\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 227 Mean Loss: 2.2724326848983765\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 228 Mean Loss: 2.0867466926574707\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 229 Mean Loss: 2.1787409484386444\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 230 Mean Loss: 2.14536602050066\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 231 Mean Loss: 2.0923091918230057\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 232 Mean Loss: 2.2136410176754\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 233 Mean Loss: 2.246689274907112\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 234 Mean Loss: 2.1086828485131264\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 235 Mean Loss: 2.1111658588051796\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 236 Mean Loss: 2.147119976580143\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 237 Mean Loss: 2.0019722431898117\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 238 Mean Loss: 2.04032601416111\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 239 Mean Loss: 2.0987873151898384\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 240 Mean Loss: 2.122587114572525\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 241 Mean Loss: 2.020104095339775\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 242 Mean Loss: 2.057154230773449\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 243 Mean Loss: 2.128930054605007\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 244 Mean Loss: 2.3128008991479874\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 245 Mean Loss: 2.242039516568184\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 246 Mean Loss: 2.09971634298563\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 247 Mean Loss: 2.1985564827919006\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 248 Mean Loss: 2.065172955393791\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 249 Mean Loss: 1.9548889994621277\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 250 Mean Loss: 1.9616564586758614\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 251 Mean Loss: 2.2157540693879128\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 252 Mean Loss: 2.3080747202038765\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 253 Mean Loss: 2.3055698424577713\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 254 Mean Loss: 2.2469643354415894\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 255 Mean Loss: 2.1911596581339836\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 256 Mean Loss: 2.26678329706192\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 257 Mean Loss: 2.370556727051735\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 258 Mean Loss: 2.2418681383132935\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 259 Mean Loss: 2.1588520258665085\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 260 Mean Loss: 1.8703766763210297\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 261 Mean Loss: 2.36530202627182\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 262 Mean Loss: 2.0909732431173325\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 263 Mean Loss: 1.9600412100553513\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 264 Mean Loss: 2.0217147171497345\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 265 Mean Loss: 1.9506136998534203\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 266 Mean Loss: 2.2917500510811806\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 267 Mean Loss: 2.1512112766504288\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 268 Mean Loss: 2.18365965038538\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 269 Mean Loss: 2.0919685512781143\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 270 Mean Loss: 2.295455679297447\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 271 Mean Loss: 2.074701152741909\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 272 Mean Loss: 1.9629354253411293\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 273 Mean Loss: 2.1156447753310204\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 274 Mean Loss: 2.2135771438479424\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 275 Mean Loss: 2.042111039161682\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 276 Mean Loss: 2.0289034619927406\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 277 Mean Loss: 2.1240966096520424\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 278 Mean Loss: 2.078167922794819\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 279 Mean Loss: 2.162910260260105\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 280 Mean Loss: 2.065874643623829\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 281 Mean Loss: 2.0130752325057983\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 282 Mean Loss: 2.1202498003840446\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 283 Mean Loss: 1.9269089996814728\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 284 Mean Loss: 1.8839913308620453\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 285 Mean Loss: 2.1868641078472137\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 286 Mean Loss: 1.9672370105981827\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 287 Mean Loss: 1.8360576927661896\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 288 Mean Loss: 2.3485719114542007\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 289 Mean Loss: 2.286215290427208\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 290 Mean Loss: 2.3607815727591515\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 291 Mean Loss: 2.0317380726337433\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 292 Mean Loss: 2.010852798819542\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 293 Mean Loss: 2.2499271482229233\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 294 Mean Loss: 2.1310317367315292\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 295 Mean Loss: 2.2664631456136703\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 296 Mean Loss: 2.0173063799738884\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 297 Mean Loss: 2.0311835557222366\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 298 Mean Loss: 2.0975001454353333\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 299 Mean Loss: 2.0534743517637253\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 300 Mean Loss: 2.1266214475035667\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 301 Mean Loss: 2.136801727116108\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 302 Mean Loss: 2.0495087951421738\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 303 Mean Loss: 2.153298795223236\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 304 Mean Loss: 2.136013127863407\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 305 Mean Loss: 2.1504915580153465\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 306 Mean Loss: 2.0140279456973076\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 307 Mean Loss: 2.143271826207638\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 308 Mean Loss: 2.2323168143630028\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 309 Mean Loss: 1.8294516429305077\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 310 Mean Loss: 2.0444607883691788\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 311 Mean Loss: 2.1168114468455315\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 312 Mean Loss: 2.1070114001631737\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 313 Mean Loss: 2.0262711569666862\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 314 Mean Loss: 1.9181309938430786\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 315 Mean Loss: 1.9083132296800613\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 316 Mean Loss: 2.2690666243433952\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 317 Mean Loss: 1.958828143775463\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 318 Mean Loss: 2.0718831196427345\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 319 Mean Loss: 2.0797369852662086\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 320 Mean Loss: 1.967277705669403\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 321 Mean Loss: 2.1154220178723335\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 322 Mean Loss: 2.08688622713089\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 323 Mean Loss: 1.9293735101819038\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 324 Mean Loss: 2.0574529469013214\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 325 Mean Loss: 1.9596944153308868\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 326 Mean Loss: 1.8918705135583878\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 327 Mean Loss: 1.9266352877020836\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 328 Mean Loss: 2.0658492669463158\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 329 Mean Loss: 2.1807807609438896\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 330 Mean Loss: 2.1337368339300156\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 331 Mean Loss: 2.2522993683815002\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 332 Mean Loss: 2.0860024839639664\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 333 Mean Loss: 1.9323513582348824\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 334 Mean Loss: 2.0941128730773926\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 335 Mean Loss: 2.1544575914740562\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 336 Mean Loss: 2.217084087431431\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 337 Mean Loss: 2.000119484961033\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 338 Mean Loss: 1.9418001845479012\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 339 Mean Loss: 2.3314631208777428\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 340 Mean Loss: 1.9891441985964775\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 341 Mean Loss: 1.9353318884968758\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 342 Mean Loss: 1.9417983144521713\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 343 Mean Loss: 2.0025202855467796\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 344 Mean Loss: 2.04244726896286\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 345 Mean Loss: 2.024573415517807\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 346 Mean Loss: 2.340533994138241\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 347 Mean Loss: 2.273444153368473\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 348 Mean Loss: 2.182192735373974\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 349 Mean Loss: 2.0180860459804535\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 350 Mean Loss: 2.143987260758877\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 351 Mean Loss: 2.224018380045891\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 352 Mean Loss: 1.9140239730477333\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 353 Mean Loss: 1.9425096586346626\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 354 Mean Loss: 2.096235543489456\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 355 Mean Loss: 1.8453675508499146\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 356 Mean Loss: 2.269768625497818\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 357 Mean Loss: 2.0157766714692116\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 358 Mean Loss: 2.078620918095112\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 359 Mean Loss: 2.036018930375576\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 360 Mean Loss: 2.098299704492092\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 361 Mean Loss: 1.968310110270977\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 362 Mean Loss: 2.0495057478547096\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 363 Mean Loss: 1.922810435295105\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 364 Mean Loss: 2.2873465269804\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 365 Mean Loss: 2.1238622218370438\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 366 Mean Loss: 2.2685625851154327\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 367 Mean Loss: 2.0347993820905685\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 368 Mean Loss: 2.2042419612407684\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 369 Mean Loss: 2.1913688480854034\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 370 Mean Loss: 1.989956684410572\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 371 Mean Loss: 1.9986751601099968\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 372 Mean Loss: 1.9027474969625473\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 373 Mean Loss: 1.9541998133063316\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 374 Mean Loss: 1.9640109241008759\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 375 Mean Loss: 2.0668706446886063\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 376 Mean Loss: 2.174449235200882\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 377 Mean Loss: 2.3214797377586365\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 378 Mean Loss: 2.039692595601082\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 379 Mean Loss: 2.267319157719612\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 380 Mean Loss: 2.1214237958192825\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 381 Mean Loss: 2.1991794258356094\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 382 Mean Loss: 2.1258654966950417\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 383 Mean Loss: 2.0959797874093056\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 384 Mean Loss: 1.9698202684521675\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 385 Mean Loss: 1.9854249209165573\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 386 Mean Loss: 2.1814967542886734\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 387 Mean Loss: 2.1274305880069733\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 388 Mean Loss: 1.9333930388092995\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 389 Mean Loss: 2.164975516498089\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 390 Mean Loss: 2.2172694355249405\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 391 Mean Loss: 2.3322383910417557\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 392 Mean Loss: 2.0615749582648277\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 393 Mean Loss: 1.9890188127756119\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 394 Mean Loss: 2.026724435389042\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 395 Mean Loss: 1.8333893939852715\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 396 Mean Loss: 2.0669183656573296\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 397 Mean Loss: 2.0416171327233315\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 398 Mean Loss: 2.013813465833664\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 399 Mean Loss: 2.173129267990589\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 400 Mean Loss: 2.026265099644661\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 401 Mean Loss: 2.06059730052948\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 402 Mean Loss: 1.9978046715259552\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 403 Mean Loss: 2.0079087018966675\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 404 Mean Loss: 2.022076480090618\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 405 Mean Loss: 2.243525981903076\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 406 Mean Loss: 2.07846899330616\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 407 Mean Loss: 2.0422462299466133\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 408 Mean Loss: 2.023826725780964\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 409 Mean Loss: 1.952484242618084\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 410 Mean Loss: 2.065312884747982\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 411 Mean Loss: 2.019650734961033\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 412 Mean Loss: 2.206387184560299\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 413 Mean Loss: 2.0031266286969185\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 414 Mean Loss: 2.513598844408989\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 415 Mean Loss: 2.0612158700823784\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 416 Mean Loss: 2.3502321392297745\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 417 Mean Loss: 2.0589682534337044\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 418 Mean Loss: 2.103531092405319\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 419 Mean Loss: 1.9963385984301567\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 420 Mean Loss: 1.989178091287613\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 421 Mean Loss: 2.1298949867486954\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 422 Mean Loss: 2.013144187629223\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 423 Mean Loss: 2.2563859969377518\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 424 Mean Loss: 2.070975288748741\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 425 Mean Loss: 2.408154845237732\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 426 Mean Loss: 2.215964697301388\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 427 Mean Loss: 2.0467821806669235\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 428 Mean Loss: 2.944871634244919\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 429 Mean Loss: 2.1472969725728035\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 430 Mean Loss: 2.4016604274511337\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 431 Mean Loss: 2.098816394805908\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 432 Mean Loss: 2.251819133758545\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 433 Mean Loss: 2.307927519083023\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 434 Mean Loss: 2.436451092362404\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 435 Mean Loss: 2.052302874624729\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 436 Mean Loss: 2.2966020554304123\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 437 Mean Loss: 2.330781042575836\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 438 Mean Loss: 2.2005050629377365\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 439 Mean Loss: 2.35188989341259\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 440 Mean Loss: 2.3115134686231613\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 441 Mean Loss: 2.2738225162029266\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 442 Mean Loss: 2.1407332345843315\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 443 Mean Loss: 2.3129874914884567\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 444 Mean Loss: 2.3090942203998566\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 445 Mean Loss: 2.129980467259884\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 446 Mean Loss: 2.3035905808210373\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 447 Mean Loss: 2.003665864467621\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 448 Mean Loss: 2.047641769051552\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 449 Mean Loss: 2.2979161590337753\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 450 Mean Loss: 2.5191441774368286\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 451 Mean Loss: 2.279838487505913\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 452 Mean Loss: 2.3307355865836143\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 453 Mean Loss: 2.2282677367329597\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 454 Mean Loss: 2.0170585364103317\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 455 Mean Loss: 2.148164249956608\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 456 Mean Loss: 2.143661253154278\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 457 Mean Loss: 2.100763864815235\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 458 Mean Loss: 2.389355257153511\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 459 Mean Loss: 2.132136754691601\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 460 Mean Loss: 2.244804121553898\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 461 Mean Loss: 2.311042584478855\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 462 Mean Loss: 2.3795364797115326\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 463 Mean Loss: 2.168486252427101\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 464 Mean Loss: 2.2112215906381607\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 465 Mean Loss: 2.1095686852931976\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 466 Mean Loss: 2.359091654419899\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 467 Mean Loss: 2.3711596578359604\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 468 Mean Loss: 2.111316353082657\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 469 Mean Loss: 1.951455980539322\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 470 Mean Loss: 2.433812841773033\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 471 Mean Loss: 2.051860734820366\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 472 Mean Loss: 1.9562035202980042\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 473 Mean Loss: 2.1713995337486267\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 474 Mean Loss: 2.2196100056171417\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 475 Mean Loss: 2.19781593978405\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 476 Mean Loss: 2.3426486253738403\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 477 Mean Loss: 1.9774794951081276\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 478 Mean Loss: 1.9828426241874695\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 479 Mean Loss: 2.1059902161359787\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 480 Mean Loss: 2.1964434385299683\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 481 Mean Loss: 2.437744438648224\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 482 Mean Loss: 2.1374195888638496\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 483 Mean Loss: 2.0811095908284187\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 484 Mean Loss: 1.9447419866919518\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 485 Mean Loss: 2.0728506222367287\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 486 Mean Loss: 1.8659879565238953\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 487 Mean Loss: 2.1114306449890137\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 488 Mean Loss: 2.2492727041244507\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 489 Mean Loss: 2.198862984776497\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 490 Mean Loss: 2.1483064144849777\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 491 Mean Loss: 2.0477918162941933\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 492 Mean Loss: 2.1141551062464714\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 493 Mean Loss: 1.9044207856059074\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 494 Mean Loss: 2.064053453505039\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 495 Mean Loss: 2.1010666862130165\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 496 Mean Loss: 2.1983224600553513\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 497 Mean Loss: 2.132871888577938\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 498 Mean Loss: 2.6227276623249054\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 499 Mean Loss: 2.548812285065651\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 500 Mean Loss: 2.01778195053339\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 501 Mean Loss: 2.0991928726434708\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 502 Mean Loss: 2.011216424405575\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 503 Mean Loss: 1.9152225703001022\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 504 Mean Loss: 2.247672274708748\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 505 Mean Loss: 1.9631007686257362\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 506 Mean Loss: 2.186407908797264\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 507 Mean Loss: 2.307542771100998\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 508 Mean Loss: 2.1461394280195236\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 509 Mean Loss: 2.002149850130081\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 510 Mean Loss: 2.044980339705944\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 511 Mean Loss: 2.234055817127228\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 512 Mean Loss: 1.9958315193653107\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 513 Mean Loss: 2.0818855687975883\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 514 Mean Loss: 2.0258108600974083\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 515 Mean Loss: 2.0960304141044617\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 516 Mean Loss: 2.045145384967327\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 517 Mean Loss: 2.323810338973999\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 518 Mean Loss: 2.099052056670189\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 519 Mean Loss: 1.9945670366287231\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 520 Mean Loss: 2.21179735660553\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 521 Mean Loss: 2.0738450437784195\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 522 Mean Loss: 2.3061375319957733\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 523 Mean Loss: 2.219101145863533\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 524 Mean Loss: 2.27885203063488\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 525 Mean Loss: 2.052228480577469\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 526 Mean Loss: 2.1766283959150314\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 527 Mean Loss: 2.388617768883705\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 528 Mean Loss: 2.423103168606758\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 529 Mean Loss: 2.19303959608078\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 530 Mean Loss: 2.362152397632599\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 531 Mean Loss: 2.1636892557144165\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 532 Mean Loss: 2.2328354492783546\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 533 Mean Loss: 1.861368104815483\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 534 Mean Loss: 2.2586541771888733\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 535 Mean Loss: 2.1464534401893616\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 536 Mean Loss: 2.1927359998226166\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 537 Mean Loss: 2.132413610816002\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 538 Mean Loss: 2.163926273584366\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 539 Mean Loss: 2.097430169582367\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 540 Mean Loss: 2.117519773542881\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 541 Mean Loss: 2.245248630642891\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 542 Mean Loss: 2.282863028347492\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 543 Mean Loss: 2.3508917838335037\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 544 Mean Loss: 2.118660695850849\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 545 Mean Loss: 2.0711198672652245\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 546 Mean Loss: 2.114358216524124\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 547 Mean Loss: 2.0568376258015633\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 548 Mean Loss: 2.093365304172039\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 549 Mean Loss: 1.9737624824047089\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 550 Mean Loss: 2.160524532198906\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 551 Mean Loss: 2.353017896413803\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 552 Mean Loss: 2.0967192128300667\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 553 Mean Loss: 2.0656416341662407\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 554 Mean Loss: 2.3238346874713898\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 555 Mean Loss: 2.2219544500112534\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 556 Mean Loss: 2.3279526233673096\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 557 Mean Loss: 2.1986176520586014\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 558 Mean Loss: 2.2841645032167435\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 559 Mean Loss: 1.9948345869779587\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 560 Mean Loss: 2.087388902902603\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 561 Mean Loss: 1.9574866220355034\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 562 Mean Loss: 2.2258414700627327\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 563 Mean Loss: 2.094882920384407\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 564 Mean Loss: 2.1204618886113167\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 565 Mean Loss: 2.178421676158905\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 566 Mean Loss: 1.8686118349432945\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 567 Mean Loss: 2.22575543820858\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 568 Mean Loss: 1.8167656883597374\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 569 Mean Loss: 1.9355168119072914\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 570 Mean Loss: 2.109762355685234\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 571 Mean Loss: 2.2074610739946365\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 572 Mean Loss: 2.085742175579071\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 573 Mean Loss: 2.19295085221529\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 574 Mean Loss: 1.877755843102932\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 575 Mean Loss: 1.7247193902730942\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 576 Mean Loss: 2.270343504846096\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 577 Mean Loss: 2.0346449315547943\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 578 Mean Loss: 2.1180084124207497\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 579 Mean Loss: 1.9358036667108536\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 580 Mean Loss: 2.3140656277537346\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 581 Mean Loss: 2.0747582837939262\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 582 Mean Loss: 2.035537511110306\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 583 Mean Loss: 2.154759645462036\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 584 Mean Loss: 2.04960697889328\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 585 Mean Loss: 2.0273330360651016\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 586 Mean Loss: 2.0566215068101883\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 587 Mean Loss: 2.0177232325077057\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 588 Mean Loss: 1.9912435784935951\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 589 Mean Loss: 2.070064537227154\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 590 Mean Loss: 2.175423853099346\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 591 Mean Loss: 2.066876143217087\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 592 Mean Loss: 1.9481830224394798\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 593 Mean Loss: 2.166621908545494\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 594 Mean Loss: 2.5260045677423477\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 595 Mean Loss: 2.3832637071609497\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 596 Mean Loss: 2.3227726742625237\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 597 Mean Loss: 2.409984916448593\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 598 Mean Loss: 2.2407071366906166\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 599 Mean Loss: 2.0674155205488205\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 600 Mean Loss: 2.349474087357521\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 601 Mean Loss: 2.471011608839035\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 602 Mean Loss: 2.4055971652269363\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 603 Mean Loss: 1.8978056758642197\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 604 Mean Loss: 2.0383154824376106\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 605 Mean Loss: 2.1146990060806274\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 606 Mean Loss: 2.2886627092957497\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 607 Mean Loss: 2.360981523990631\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 608 Mean Loss: 2.1251808181405067\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 609 Mean Loss: 2.2881265580654144\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 610 Mean Loss: 2.222162388265133\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 611 Mean Loss: 2.2680691927671432\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 612 Mean Loss: 2.178470991551876\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 613 Mean Loss: 2.059173382818699\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 614 Mean Loss: 1.969192236661911\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 615 Mean Loss: 2.1991192549467087\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 616 Mean Loss: 1.985007993876934\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 617 Mean Loss: 2.5873800963163376\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 618 Mean Loss: 1.974227987229824\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 619 Mean Loss: 2.1116318330168724\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 620 Mean Loss: 2.1651342138648033\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 621 Mean Loss: 2.3253495395183563\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 622 Mean Loss: 2.2875653505325317\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 623 Mean Loss: 2.3630810230970383\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 624 Mean Loss: 2.1250782907009125\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 625 Mean Loss: 1.9578712061047554\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 626 Mean Loss: 2.1903592199087143\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 627 Mean Loss: 2.17961922287941\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 628 Mean Loss: 2.0300323963165283\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 629 Mean Loss: 2.196500688791275\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 630 Mean Loss: 2.2945642471313477\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 631 Mean Loss: 2.3700848519802094\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 632 Mean Loss: 2.4461433589458466\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 633 Mean Loss: 2.400801107287407\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 634 Mean Loss: 2.123621605336666\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 635 Mean Loss: 2.471700042486191\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 636 Mean Loss: 2.147836521267891\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 637 Mean Loss: 1.967201516032219\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 638 Mean Loss: 2.1161782890558243\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 639 Mean Loss: 2.1301101744174957\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 640 Mean Loss: 2.067910313606262\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 641 Mean Loss: 2.033595159649849\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 642 Mean Loss: 1.9987455308437347\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 643 Mean Loss: 2.286428213119507\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 644 Mean Loss: 2.329890727996826\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 645 Mean Loss: 2.3308182656764984\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 646 Mean Loss: 2.3493781834840775\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 647 Mean Loss: 2.1398985385894775\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 648 Mean Loss: 2.2157186418771744\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 649 Mean Loss: 2.1619470939040184\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 650 Mean Loss: 2.3192534297704697\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 651 Mean Loss: 2.096562646329403\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 652 Mean Loss: 2.119177646934986\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 653 Mean Loss: 2.2342588752508163\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 654 Mean Loss: 2.084571845829487\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 655 Mean Loss: 2.079240694642067\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 656 Mean Loss: 2.377169519662857\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 657 Mean Loss: 2.1919872611761093\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 658 Mean Loss: 2.514536142349243\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 659 Mean Loss: 2.212448053061962\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 660 Mean Loss: 2.2450847029685974\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 661 Mean Loss: 1.9118268191814423\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 662 Mean Loss: 2.039833255112171\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 663 Mean Loss: 2.2061145156621933\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 664 Mean Loss: 2.414085954427719\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 665 Mean Loss: 2.176378905773163\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 666 Mean Loss: 2.0220368206501007\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 667 Mean Loss: 2.2153792083263397\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 668 Mean Loss: 2.2600781098008156\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 669 Mean Loss: 2.4166298657655716\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 670 Mean Loss: 2.0827127397060394\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 671 Mean Loss: 2.074374444782734\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 672 Mean Loss: 2.2425246238708496\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 673 Mean Loss: 2.0153135806322098\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 674 Mean Loss: 2.3052551671862602\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 675 Mean Loss: 2.369405969977379\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 676 Mean Loss: 2.4436731785535812\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 677 Mean Loss: 2.1917140632867813\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 678 Mean Loss: 2.0154160633683205\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 679 Mean Loss: 2.0729705542325974\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 680 Mean Loss: 2.14082670211792\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 681 Mean Loss: 1.986263781785965\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 682 Mean Loss: 2.2088047340512276\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 683 Mean Loss: 2.0575801953673363\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 684 Mean Loss: 1.9355857893824577\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 685 Mean Loss: 2.065616711974144\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 686 Mean Loss: 2.0143456906080246\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 687 Mean Loss: 2.004787005484104\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 688 Mean Loss: 2.2342482805252075\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 689 Mean Loss: 2.166658528149128\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 690 Mean Loss: 2.021107129752636\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 691 Mean Loss: 2.1722891852259636\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 692 Mean Loss: 2.0753290876746178\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 693 Mean Loss: 1.9639267101883888\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 694 Mean Loss: 2.087763249874115\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 695 Mean Loss: 2.124258369207382\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 696 Mean Loss: 1.9656902477145195\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 697 Mean Loss: 2.1792607232928276\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 698 Mean Loss: 2.258597083389759\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 699 Mean Loss: 1.9644673466682434\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 700 Mean Loss: 2.074869453907013\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 701 Mean Loss: 2.280233532190323\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 702 Mean Loss: 1.9276791661977768\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 703 Mean Loss: 2.4922637045383453\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 704 Mean Loss: 2.220197342336178\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 705 Mean Loss: 2.2081906646490097\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 706 Mean Loss: 2.037130519747734\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 707 Mean Loss: 2.298560246825218\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 708 Mean Loss: 2.227502480149269\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 709 Mean Loss: 1.973245158791542\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 710 Mean Loss: 2.1830903440713882\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 711 Mean Loss: 2.148403987288475\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 712 Mean Loss: 2.0459514781832695\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 713 Mean Loss: 1.9769437238574028\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 714 Mean Loss: 2.103324234485626\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 715 Mean Loss: 2.008832834661007\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 716 Mean Loss: 2.1137301102280617\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 717 Mean Loss: 1.946237027645111\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 718 Mean Loss: 2.358724147081375\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 719 Mean Loss: 2.0505676716566086\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 720 Mean Loss: 1.9951697066426277\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 721 Mean Loss: 2.0448376163840294\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 722 Mean Loss: 1.9256910681724548\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 723 Mean Loss: 2.0497256070375443\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 724 Mean Loss: 1.9827233776450157\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 725 Mean Loss: 1.974708929657936\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 726 Mean Loss: 1.9308133125305176\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 727 Mean Loss: 2.0811216458678246\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 728 Mean Loss: 2.4032182842493057\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 729 Mean Loss: 2.1705595701932907\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 730 Mean Loss: 1.9941071718931198\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 731 Mean Loss: 2.360370360314846\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 732 Mean Loss: 2.189961798489094\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 733 Mean Loss: 2.2301070764660835\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 734 Mean Loss: 1.9642736241221428\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 735 Mean Loss: 2.3361863270401955\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 736 Mean Loss: 2.334299720823765\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 737 Mean Loss: 2.300044633448124\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 738 Mean Loss: 2.13051750510931\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 739 Mean Loss: 2.1405677124857903\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 740 Mean Loss: 2.2889174073934555\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 741 Mean Loss: 1.9362629652023315\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 742 Mean Loss: 2.166875936090946\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 743 Mean Loss: 2.220633804798126\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 744 Mean Loss: 2.1737740337848663\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 745 Mean Loss: 2.2890704870224\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 746 Mean Loss: 2.2221365347504616\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 747 Mean Loss: 2.3250450491905212\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 748 Mean Loss: 2.0969959422945976\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 749 Mean Loss: 2.1196195110678673\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 750 Mean Loss: 2.1537269726395607\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 751 Mean Loss: 2.267452910542488\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 752 Mean Loss: 2.4240723848342896\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 753 Mean Loss: 2.1089767813682556\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 754 Mean Loss: 2.353005051612854\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 755 Mean Loss: 2.1335160434246063\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 756 Mean Loss: 2.059506520628929\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 757 Mean Loss: 2.494492083787918\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 758 Mean Loss: 2.2783480435609818\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 759 Mean Loss: 2.2124080285429955\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 760 Mean Loss: 2.3429739996790886\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 761 Mean Loss: 2.359213560819626\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 762 Mean Loss: 2.219215229153633\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 763 Mean Loss: 2.172292284667492\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 764 Mean Loss: 2.189755007624626\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 765 Mean Loss: 2.2860213965177536\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 766 Mean Loss: 2.2444984763860703\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 767 Mean Loss: 2.2505041286349297\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 768 Mean Loss: 2.188266709446907\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 769 Mean Loss: 2.315290153026581\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 770 Mean Loss: 2.2383532375097275\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 771 Mean Loss: 2.1150101125240326\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 772 Mean Loss: 2.245634488761425\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 773 Mean Loss: 2.221482001245022\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 774 Mean Loss: 2.090843580663204\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 775 Mean Loss: 2.1937631890177727\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 776 Mean Loss: 3.289662942290306\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 777 Mean Loss: 2.1010505482554436\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 778 Mean Loss: 2.141414113342762\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 779 Mean Loss: 2.332447499036789\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 780 Mean Loss: 1.977001667022705\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 781 Mean Loss: 2.1243099197745323\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 782 Mean Loss: 2.2209813743829727\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 783 Mean Loss: 2.32519718259573\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 784 Mean Loss: 1.987724743783474\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 785 Mean Loss: 2.1712952256202698\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 786 Mean Loss: 2.236683316528797\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 787 Mean Loss: 2.181578256189823\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 788 Mean Loss: 2.1143497303128242\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 789 Mean Loss: 2.1439920514822006\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 790 Mean Loss: 2.0746567249298096\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 791 Mean Loss: 2.394329622387886\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 792 Mean Loss: 1.9245492294430733\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 793 Mean Loss: 2.0129855573177338\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 794 Mean Loss: 2.38162162899971\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 795 Mean Loss: 2.3072292804718018\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 796 Mean Loss: 2.002344198524952\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 797 Mean Loss: 2.3506923764944077\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 798 Mean Loss: 2.2266077250242233\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 799 Mean Loss: 2.1403967663645744\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 800 Mean Loss: 2.360749676823616\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 801 Mean Loss: 2.1341401040554047\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 802 Mean Loss: 1.97508754581213\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 803 Mean Loss: 2.1337799727916718\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 804 Mean Loss: 2.178062967956066\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 805 Mean Loss: 2.427498683333397\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 806 Mean Loss: 2.091069407761097\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 807 Mean Loss: 2.2269138246774673\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 808 Mean Loss: 2.1971491128206253\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 809 Mean Loss: 2.131159596145153\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 810 Mean Loss: 2.0937129259109497\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 811 Mean Loss: 2.2774822041392326\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 812 Mean Loss: 2.002591349184513\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 813 Mean Loss: 2.23318287730217\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 814 Mean Loss: 2.270833060145378\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 815 Mean Loss: 2.1308531537652016\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 816 Mean Loss: 2.0537476167082787\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 817 Mean Loss: 2.022193230688572\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 818 Mean Loss: 2.3833090513944626\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 819 Mean Loss: 2.0019848570227623\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 820 Mean Loss: 2.1680472046136856\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 821 Mean Loss: 2.255374141037464\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 822 Mean Loss: 2.124366022646427\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 823 Mean Loss: 2.295198619365692\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 824 Mean Loss: 2.0127448588609695\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 825 Mean Loss: 2.1178930550813675\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 826 Mean Loss: 2.3387003540992737\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 827 Mean Loss: 2.3993562310934067\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 828 Mean Loss: 2.378636859357357\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 829 Mean Loss: 2.6308217346668243\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 830 Mean Loss: 2.1955105289816856\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 831 Mean Loss: 2.408984035253525\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 832 Mean Loss: 2.523205265402794\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 833 Mean Loss: 2.289680451154709\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 834 Mean Loss: 2.4375985860824585\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 835 Mean Loss: 2.428348481655121\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 836 Mean Loss: 2.2002669274806976\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 837 Mean Loss: 2.502270832657814\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 838 Mean Loss: 2.391089081764221\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 839 Mean Loss: 2.464538961648941\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 840 Mean Loss: 2.331825837492943\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 841 Mean Loss: 2.1150408163666725\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 842 Mean Loss: 2.3347813189029694\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 843 Mean Loss: 2.3457196950912476\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 844 Mean Loss: 2.2877281308174133\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 845 Mean Loss: 2.1730921864509583\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 846 Mean Loss: 2.3859949558973312\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 847 Mean Loss: 2.411079928278923\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 848 Mean Loss: 2.34352146089077\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 849 Mean Loss: 2.148700535297394\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 850 Mean Loss: 2.340971164405346\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 851 Mean Loss: 2.23542071133852\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 852 Mean Loss: 2.339092940092087\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 853 Mean Loss: 2.3778792917728424\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 854 Mean Loss: 2.531931146979332\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 855 Mean Loss: 2.3985746055841446\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 856 Mean Loss: 2.4113736003637314\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 857 Mean Loss: 2.2948518246412277\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 858 Mean Loss: 2.3774339109659195\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 859 Mean Loss: 2.267240509390831\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 860 Mean Loss: 2.5454301238059998\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 861 Mean Loss: 2.3052659183740616\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 862 Mean Loss: 2.3354958444833755\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 863 Mean Loss: 2.4205398559570312\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 864 Mean Loss: 2.321330204606056\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 865 Mean Loss: 2.2346069291234016\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 866 Mean Loss: 2.1812369972467422\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 867 Mean Loss: 2.044207885861397\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 868 Mean Loss: 2.1593725830316544\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 869 Mean Loss: 2.11135932803154\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 870 Mean Loss: 2.42369444668293\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 871 Mean Loss: 2.392069175839424\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 872 Mean Loss: 2.2618744522333145\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 873 Mean Loss: 1.9004868939518929\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 874 Mean Loss: 2.3214099034667015\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 875 Mean Loss: 2.350222483277321\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 876 Mean Loss: 2.000156119465828\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 877 Mean Loss: 2.364980027079582\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 878 Mean Loss: 2.2762852907180786\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 879 Mean Loss: 1.9969087541103363\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 880 Mean Loss: 2.1729191318154335\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 881 Mean Loss: 2.18115908652544\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 882 Mean Loss: 2.275499649345875\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 883 Mean Loss: 2.287959575653076\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 884 Mean Loss: 2.508040353655815\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 885 Mean Loss: 2.19868291169405\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 886 Mean Loss: 2.558831050992012\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 887 Mean Loss: 2.2384188920259476\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 888 Mean Loss: 2.105633370578289\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 889 Mean Loss: 2.3548123091459274\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 890 Mean Loss: 2.3229449540376663\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 891 Mean Loss: 2.020002171397209\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 892 Mean Loss: 2.0957697927951813\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 893 Mean Loss: 2.4276002645492554\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 894 Mean Loss: 2.2070365622639656\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 895 Mean Loss: 1.9658691585063934\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 896 Mean Loss: 1.9926050901412964\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 897 Mean Loss: 1.9565121158957481\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 898 Mean Loss: 1.9812035635113716\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 899 Mean Loss: 2.079624764621258\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 900 Mean Loss: 2.410612776875496\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 901 Mean Loss: 2.4230146408081055\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 902 Mean Loss: 2.1882642805576324\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 903 Mean Loss: 2.115687184035778\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 904 Mean Loss: 2.405277669429779\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 905 Mean Loss: 2.309216685593128\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 906 Mean Loss: 2.1280001401901245\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 907 Mean Loss: 2.046639807522297\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 908 Mean Loss: 2.016555368900299\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 909 Mean Loss: 2.1208673045039177\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 910 Mean Loss: 2.376407414674759\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 911 Mean Loss: 2.392678216099739\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 912 Mean Loss: 2.3049923181533813\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 913 Mean Loss: 2.4581931233406067\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 914 Mean Loss: 2.1856186538934708\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 915 Mean Loss: 2.1992683708667755\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 916 Mean Loss: 2.3065730184316635\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 917 Mean Loss: 2.2550397366285324\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 918 Mean Loss: 2.096740759909153\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 919 Mean Loss: 2.315762162208557\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 920 Mean Loss: 2.231274515390396\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 921 Mean Loss: 2.453748643398285\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 922 Mean Loss: 2.3027657717466354\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 923 Mean Loss: 1.9255471155047417\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 924 Mean Loss: 2.2535443902015686\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 925 Mean Loss: 2.237017512321472\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 926 Mean Loss: 2.168496496975422\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 927 Mean Loss: 2.2620304822921753\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 928 Mean Loss: 2.21149168163538\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 929 Mean Loss: 2.203978218138218\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 930 Mean Loss: 2.3563505560159683\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 931 Mean Loss: 2.0405282750725746\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 932 Mean Loss: 2.3654032796621323\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 933 Mean Loss: 2.165183335542679\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 934 Mean Loss: 2.2690785974264145\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 935 Mean Loss: 2.361273370683193\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 936 Mean Loss: 2.329128012061119\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 937 Mean Loss: 2.4813326448202133\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 938 Mean Loss: 2.363108992576599\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 939 Mean Loss: 2.21268393099308\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 940 Mean Loss: 2.2898228839039803\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 941 Mean Loss: 2.3802641481161118\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 942 Mean Loss: 2.036147564649582\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 943 Mean Loss: 2.3044620007276535\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 944 Mean Loss: 2.1174088418483734\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 945 Mean Loss: 2.3707622438669205\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 946 Mean Loss: 2.197215087711811\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 947 Mean Loss: 2.181502565741539\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 948 Mean Loss: 2.2970002442598343\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 949 Mean Loss: 2.169938340783119\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 950 Mean Loss: 2.1462507396936417\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 951 Mean Loss: 2.2123521491885185\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 952 Mean Loss: 1.977461338043213\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 953 Mean Loss: 2.502685934305191\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 954 Mean Loss: 2.32922226190567\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 955 Mean Loss: 2.2386002019047737\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 956 Mean Loss: 2.355099156498909\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 957 Mean Loss: 2.5147374719381332\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 958 Mean Loss: 2.3603519946336746\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 959 Mean Loss: 2.2178341671824455\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 960 Mean Loss: 2.235739804804325\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 961 Mean Loss: 2.327720530331135\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 962 Mean Loss: 1.96378755569458\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 963 Mean Loss: 2.032807908952236\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 964 Mean Loss: 2.1120138466358185\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 965 Mean Loss: 2.3165863156318665\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 966 Mean Loss: 2.21335019916296\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 967 Mean Loss: 2.145964600145817\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 968 Mean Loss: 2.016492336988449\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 969 Mean Loss: 1.999354898929596\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 970 Mean Loss: 2.2372449412941933\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 971 Mean Loss: 2.269180044531822\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 972 Mean Loss: 2.0576165914535522\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 973 Mean Loss: 2.4331779330968857\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 974 Mean Loss: 2.1241260319948196\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 975 Mean Loss: 2.091310538351536\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 976 Mean Loss: 2.1479362398386\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 977 Mean Loss: 2.0496838241815567\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 978 Mean Loss: 2.136582314968109\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 979 Mean Loss: 2.1908835768699646\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 980 Mean Loss: 2.2226661518216133\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 981 Mean Loss: 2.2445195615291595\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 982 Mean Loss: 2.018234945833683\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 983 Mean Loss: 2.3130434677004814\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 984 Mean Loss: 2.1907948181033134\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 985 Mean Loss: 1.9479405134916306\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 986 Mean Loss: 2.2149058431386948\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 987 Mean Loss: 2.3408321142196655\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 988 Mean Loss: 2.1513158455491066\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 989 Mean Loss: 2.123268701136112\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 990 Mean Loss: 2.2848635837435722\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 991 Mean Loss: 2.2232835441827774\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 992 Mean Loss: 2.0822912603616714\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 993 Mean Loss: 2.3253876119852066\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 994 Mean Loss: 2.962658628821373\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 995 Mean Loss: 2.296821780502796\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 996 Mean Loss: 2.2231596410274506\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 997 Mean Loss: 2.074586942791939\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 998 Mean Loss: 2.344663456082344\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 999 Mean Loss: 2.3837295174598694\n",
            "mean reward:  tensor(1024.)\n",
            "Rollout 1000 Mean Loss: 2.1927280947566032\n"
          ]
        }
      ],
      "source": [
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"CartPole-v1\" # going to use \"CartPole-v1\"\n",
        "clipping_coef = 0.1\n",
        "num_envs = 4\n",
        "rollouts = 1000\n",
        "middle_layer_size = 64 # Define the size of the middle layer\n",
        "batch_size = 256 # Define a batch size for reshaping\n",
        "num_epochs = 4\n",
        "num_mini_batches = 4\n",
        "clipping_coef = 0.2\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "# Conventional Vectorized Environment wrapper\n",
        "def make_env(env_id, seed=None): # Added seed parameter\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        if seed is not None: # Set seed if provided\n",
        "            env.action_space.seed(seed)\n",
        "            env.observation_space.seed(seed)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, observation_space_shape, action_space_size, middle_layer_size) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      # Actor/Policy\n",
        "      self.actor = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, action_space_size),\n",
        "          nn.Softmax(dim=-1)\n",
        "          ) # Added dim=-1 to softmax\n",
        "\n",
        "      # Critic/Advantage NN //might need another activation function at the end.\n",
        "      self.critic = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, 1), # Output size of 1 for the value function\n",
        "          )\n",
        "\n",
        "  def predict(self, x):\n",
        "    action_probs = self.actor(x)\n",
        "    act_dist = Categorical(action_probs)\n",
        "    action = act_dist.sample()\n",
        "    log_prob = act_dist.log_prob(action)\n",
        "    entropy = act_dist.entropy() # Calculate entropy\n",
        "\n",
        "    value_logits = self.critic(x)\n",
        "\n",
        "    # return entropy, probabilies, and sampled action\n",
        "    return (entropy, log_prob, action, value_logits) # Return entropy, probabilities, and a sampled action\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Corrected __main__\n",
        "\n",
        "  # initilizattion\n",
        "  envs = gym.vector.AsyncVectorEnv([make_env(env_id, seed=i) for i in range(num_envs)]) # Pass individual seeds\n",
        "\n",
        "  # Get observation and action space dimensions\n",
        "  observation_space_shape = envs.single_observation_space.shape[0] # Assuming flat observation space\n",
        "  action_space_size = envs.single_action_space.n # Assuming discrete action space\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #initialize the Agent\n",
        "  agent = Agent(observation_space_shape, action_space_size, middle_layer_size).to(device) # Pass dimensions and move to device\n",
        "\n",
        "  #initialize the optimizer\n",
        "  optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
        "\n",
        "  # Initialize tensors with appropriate shapes\n",
        "  obs = torch.zeros((batch_size, num_envs, observation_space_shape)).to(device)\n",
        "  actions = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  logprobs = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  rewards = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  dones = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  pred_values = torch.zeros((batch_size, num_envs)).to(device)\n",
        "\n",
        "  # init actual values and advantages tensor\n",
        "  actual_values = torch.zeros_like(rewards).to(device)\n",
        "  advantages = torch.zeros_like(rewards).to(device)\n",
        "\n",
        "\n",
        "  # initializes the observation, done, the time, and the step\n",
        "  start_time = time.time()\n",
        "  global_step = 0\n",
        "\n",
        "  # define training regime\n",
        "  for i in range(int(rollouts)): # Cast steps to int\n",
        "    step = 0 # Initialize step counter for batch\n",
        "    next_obs = torch.Tensor(envs.reset()).to(device) # Corrected envs.reset()\n",
        "    next_done =  torch.zeros((num_envs,)).to(device)\n",
        "\n",
        "    for step in range(int(batch_size)):\n",
        "      # get actions, observations, rewards, and dones\n",
        "      with torch.no_grad(): # Added no_grad for inference\n",
        "          _, log_prob, action, values_ = agent.predict(next_obs) # Renamed values to values_ to avoid conflict\n",
        "\n",
        "      # Move data to tensors\n",
        "      next_obs_np, rewards_np, next_done_np, infos =  envs.step(action.cpu().numpy()) # env step and move action to cpu\n",
        "\n",
        "      # Moves things that were on the cpu onto the gpu\n",
        "      next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "      next_done = torch.Tensor(next_done_np).to(device)\n",
        "      reward = torch.Tensor(rewards_np).to(device)\n",
        "\n",
        "      # Store data in tensors at the current step\n",
        "      obs[step] = next_obs\n",
        "      actions[step] = action\n",
        "      logprobs[step] = log_prob.detach()\n",
        "      rewards[step] = reward\n",
        "      dones[step] = next_done\n",
        "      pred_values[step] = values_.squeeze(-1).detach() # Remove the last dimension of size 1\n",
        "\n",
        "      global_step += num_envs # Update global step\n",
        "\n",
        "    # calculate actual values at each time step'\n",
        "    print(\"mean reward: \", rewards.sum().cpu())\n",
        "\n",
        "    with torch.no_grad(): # Calculate advantages outside the gradient tape\n",
        "        for t in reversed(range(batch_size)):\n",
        "          if t == batch_size - 1:\n",
        "              # For the last step, if the environment is not done, use the value of the next state (from the agent's prediction)\n",
        "              # Otherwise, the actual value is just the reward at this step\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              next_value = agent.critic(next_obs).squeeze(-1).detach() # bootstrap next value since it doesn't exsist\n",
        "          else:\n",
        "              # For other steps, if the environment at the next step is not done, use the value of the next state from the stored values\n",
        "              # Otherwise, the actual value is just the reward at this step\n",
        "              nextnonterminal = 1.0 - dones[t+1]\n",
        "              next_value = actual_values[t+1]\n",
        "          actual_values[t] = rewards[t] + gamma * next_value * nextnonterminal\n",
        "        advantages = actual_values - pred_values.detach() # Detach pred_values here\n",
        "\n",
        "\n",
        "    # Actually training the agent neural net\n",
        "\n",
        "    # flattening the tensors for ease\n",
        "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "    b_logprobs = logprobs.reshape(-1)\n",
        "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "    b_advantages = advantages.reshape(-1)\n",
        "    b_actual_values = actual_values.reshape(-1)\n",
        "    b_pred_values = pred_values.reshape(-1)\n",
        "\n",
        "    # creates storage to see loss over time\n",
        "    rollout_losses = [] # Initialize list to store losses for the current rollout\n",
        "\n",
        "    # Iterates over the same batch a couple times for efficiency\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      #seperates into minibatches\n",
        "      indices = np.arange(batch_size * num_envs)   # creates indicies\n",
        "      np.random.shuffle(indices)        # shuffles indicies\n",
        "      minibatch_indices = np.array_split(indices, num_mini_batches)\n",
        "\n",
        "      #iterates over the minibatches\n",
        "      for current_minibatch_indices in minibatch_indices: # Corrected indexing for minibatch_indices\n",
        "\n",
        "        # init mini_batch\n",
        "        mb_obs = b_obs[current_minibatch_indices]\n",
        "        mb_log_probs = b_logprobs[current_minibatch_indices]\n",
        "        mb_actions = b_actions[current_minibatch_indices] # Corrected indexing for mb_actions\n",
        "        mb_advantages = b_advantages[current_minibatch_indices]\n",
        "        mb_actual_values = b_actual_values[current_minibatch_indices]\n",
        "        mb_pred_values = b_pred_values[current_minibatch_indices].detach() # Detach old predicted values for clipping\n",
        "\n",
        "\n",
        "        # get new logprobs(but don't overwrite), values, and entropy\n",
        "        mb_new_entropy, mb_new_log_probs, _, mb_new_values_ = agent.predict(mb_obs) # note: may need to be flattened\n",
        "\n",
        "        # value optimization\n",
        "        unclipped_value_loss = (mb_actual_values - mb_new_values_.squeeze(-1)) ** 2\n",
        "\n",
        "        clipped_predicted_values = mb_pred_values + torch.clamp(mb_new_values_.squeeze()- mb_pred_values, -clipping_coef, clipping_coef)\n",
        "        clipped_value_Loss = (mb_actual_values - clipped_predicted_values ) ** 2\n",
        "\n",
        "        value_loss = torch.max(unclipped_value_loss, clipped_value_Loss).mean()\n",
        "\n",
        "        # policy optimization\n",
        "\n",
        "        # Normalize advantages\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8) # Add a small epsilon for numerical stability\n",
        "\n",
        "        # calculate ratios\n",
        "        unclipped_ratio = (mb_new_log_probs - mb_log_probs).exp()\n",
        "        clipped_ratio = torch.clamp(unclipped_ratio, 1 - clipping_coef, 1 + clipping_coef)\n",
        "\n",
        "        # calculate loss\n",
        "        policy_loss = torch.max(-mb_advantages*unclipped_ratio, -mb_advantages*clipped_ratio).mean()\n",
        "\n",
        "        # calculates entropy\n",
        "        entropy_loss = mb_new_entropy.mean()\n",
        "\n",
        "        #calculates total loss\n",
        "\n",
        "        loss = policy_loss - (entropy_loss*0.01) + (value_loss * 0.5)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        rollout_losses.append(loss.item()) # Append mini-batch loss to list\n",
        "\n",
        "    # Calculate and print mean loss for the current rollout\n",
        "    mean_rollout_loss = np.mean(rollout_losses)\n",
        "    print(f\"Rollout {i+1} Mean Loss: {mean_rollout_loss}\")\n",
        "\n",
        "\n",
        "    # This part of the code needs to be implemented for the training loop (calculating advantages, updating networks, etc.)\n",
        "    # This is a placeholder and would typically involve:\n",
        "    # 1. Calculating advantages/returns\n",
        "    # 2. Calculating policy and value losses\n",
        "    # 3. Performing backpropagation and optimizer steps\n",
        "\n",
        "  envs.close() # Close the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo2bN-hrJ5yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc48b610",
        "outputId": "c2c0fc6c-2207-4e11-a517-a168224a6cfb"
      },
      "source": [
        "# Set up evaluation environment\n",
        "eval_env = gym.make(env_id, render_mode='rgb_array') # Use render_mode for video recording\n",
        "\n",
        "# Optional: Wrap the environment to record video\n",
        "# You might need to install 'moviepy' and 'ffmpeg' for this.\n",
        "# !pip install moviepy ffmpeg\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "# Create a directory to save videos\n",
        "video_folder = \"./eval_videos\"\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "eval_env = RecordVideo(eval_env, video_folder)\n",
        "\n",
        "# Load the trained agent's state (assuming agent is still in memory or saved)\n",
        "# If you saved the agent, you would load it here:\n",
        "# agent.load_state_dict(torch.load(\"path/to/your/agent.pth\"))\n",
        "\n",
        "# Set agent to evaluation mode\n",
        "agent.eval()\n",
        "\n",
        "# Run evaluation episodes\n",
        "num_eval_episodes = 100 # Number of episodes for evaluation\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_eval_episodes):\n",
        "    obs= eval_env.reset() # Correctly unpack observation and info\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        with torch.no_grad(): # Use no_grad for inference\n",
        "            # Convert observation to tensor and move to device\n",
        "            obs_tensor = torch.Tensor(obs).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "            # Get action from the agent (use predict for single environment inference)\n",
        "            _, _, action, _ = agent.predict(obs_tensor)\n",
        "\n",
        "            # Remove batch dimension and move action to cpu for environment step\n",
        "            # For a single discrete action, get the scalar value\n",
        "            action_np = action.squeeze(0).cpu().numpy().item()\n",
        "\n",
        "\n",
        "        # Step the environment\n",
        "        obs, reward, terminated, truncated = eval_env.step(action_np) # Correctly unpack all 5 values\n",
        "        episode_reward += reward\n",
        "        done = terminated or truncated # Consider either terminated or truncated as done for episode termination\n",
        "\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward}\")\n",
        "\n",
        "# Close the evaluation environment\n",
        "# Attempt to close the underlying environment directly as a workaround for potential wrapper close issues\n",
        "eval_env.env.close()\n",
        "\n",
        "\n",
        "# Calculate and print mean reward\n",
        "mean_eval_reward = np.mean(episode_rewards)\n",
        "print(f\"\\nMean Evaluation Reward over {num_eval_episodes} episodes: {mean_eval_reward}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/eval_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Episode 1: Reward = 14.0\n",
            "Evaluation Episode 2: Reward = 18.0\n",
            "Evaluation Episode 3: Reward = 33.0\n",
            "Evaluation Episode 4: Reward = 29.0\n",
            "Evaluation Episode 5: Reward = 14.0\n",
            "Evaluation Episode 6: Reward = 40.0\n",
            "Evaluation Episode 7: Reward = 17.0\n",
            "Evaluation Episode 8: Reward = 11.0\n",
            "Evaluation Episode 9: Reward = 22.0\n",
            "Evaluation Episode 10: Reward = 22.0\n",
            "Evaluation Episode 11: Reward = 14.0\n",
            "Evaluation Episode 12: Reward = 11.0\n",
            "Evaluation Episode 13: Reward = 30.0\n",
            "Evaluation Episode 14: Reward = 16.0\n",
            "Evaluation Episode 15: Reward = 17.0\n",
            "Evaluation Episode 16: Reward = 16.0\n",
            "Evaluation Episode 17: Reward = 42.0\n",
            "Evaluation Episode 18: Reward = 14.0\n",
            "Evaluation Episode 19: Reward = 21.0\n",
            "Evaluation Episode 20: Reward = 23.0\n",
            "Evaluation Episode 21: Reward = 12.0\n",
            "Evaluation Episode 22: Reward = 33.0\n",
            "Evaluation Episode 23: Reward = 16.0\n",
            "Evaluation Episode 24: Reward = 11.0\n",
            "Evaluation Episode 25: Reward = 16.0\n",
            "Evaluation Episode 26: Reward = 16.0\n",
            "Evaluation Episode 27: Reward = 26.0\n",
            "Evaluation Episode 28: Reward = 30.0\n",
            "Evaluation Episode 29: Reward = 12.0\n",
            "Evaluation Episode 30: Reward = 10.0\n",
            "Evaluation Episode 31: Reward = 31.0\n",
            "Evaluation Episode 32: Reward = 14.0\n",
            "Evaluation Episode 33: Reward = 13.0\n",
            "Evaluation Episode 34: Reward = 13.0\n",
            "Evaluation Episode 35: Reward = 17.0\n",
            "Evaluation Episode 36: Reward = 19.0\n",
            "Evaluation Episode 37: Reward = 26.0\n",
            "Evaluation Episode 38: Reward = 16.0\n",
            "Evaluation Episode 39: Reward = 13.0\n",
            "Evaluation Episode 40: Reward = 14.0\n",
            "Evaluation Episode 41: Reward = 14.0\n",
            "Evaluation Episode 42: Reward = 27.0\n",
            "Evaluation Episode 43: Reward = 17.0\n",
            "Evaluation Episode 44: Reward = 16.0\n",
            "Evaluation Episode 45: Reward = 19.0\n",
            "Evaluation Episode 46: Reward = 39.0\n",
            "Evaluation Episode 47: Reward = 53.0\n",
            "Evaluation Episode 48: Reward = 31.0\n",
            "Evaluation Episode 49: Reward = 10.0\n",
            "Evaluation Episode 50: Reward = 62.0\n",
            "Evaluation Episode 51: Reward = 15.0\n",
            "Evaluation Episode 52: Reward = 18.0\n",
            "Evaluation Episode 53: Reward = 41.0\n",
            "Evaluation Episode 54: Reward = 23.0\n",
            "Evaluation Episode 55: Reward = 50.0\n",
            "Evaluation Episode 56: Reward = 11.0\n",
            "Evaluation Episode 57: Reward = 20.0\n",
            "Evaluation Episode 58: Reward = 43.0\n",
            "Evaluation Episode 59: Reward = 15.0\n",
            "Evaluation Episode 60: Reward = 13.0\n",
            "Evaluation Episode 61: Reward = 24.0\n",
            "Evaluation Episode 62: Reward = 14.0\n",
            "Evaluation Episode 63: Reward = 20.0\n",
            "Evaluation Episode 64: Reward = 16.0\n",
            "Evaluation Episode 65: Reward = 21.0\n",
            "Evaluation Episode 66: Reward = 12.0\n",
            "Evaluation Episode 67: Reward = 25.0\n",
            "Evaluation Episode 68: Reward = 15.0\n",
            "Evaluation Episode 69: Reward = 36.0\n",
            "Evaluation Episode 70: Reward = 11.0\n",
            "Evaluation Episode 71: Reward = 8.0\n",
            "Evaluation Episode 72: Reward = 27.0\n",
            "Evaluation Episode 73: Reward = 16.0\n",
            "Evaluation Episode 74: Reward = 34.0\n",
            "Evaluation Episode 75: Reward = 11.0\n",
            "Evaluation Episode 76: Reward = 17.0\n",
            "Evaluation Episode 77: Reward = 20.0\n",
            "Evaluation Episode 78: Reward = 14.0\n",
            "Evaluation Episode 79: Reward = 16.0\n",
            "Evaluation Episode 80: Reward = 13.0\n",
            "Evaluation Episode 81: Reward = 11.0\n",
            "Evaluation Episode 82: Reward = 13.0\n",
            "Evaluation Episode 83: Reward = 29.0\n",
            "Evaluation Episode 84: Reward = 10.0\n",
            "Evaluation Episode 85: Reward = 15.0\n",
            "Evaluation Episode 86: Reward = 15.0\n",
            "Evaluation Episode 87: Reward = 27.0\n",
            "Evaluation Episode 88: Reward = 10.0\n",
            "Evaluation Episode 89: Reward = 17.0\n",
            "Evaluation Episode 90: Reward = 26.0\n",
            "Evaluation Episode 91: Reward = 12.0\n",
            "Evaluation Episode 92: Reward = 36.0\n",
            "Evaluation Episode 93: Reward = 17.0\n",
            "Evaluation Episode 94: Reward = 25.0\n",
            "Evaluation Episode 95: Reward = 25.0\n",
            "Evaluation Episode 96: Reward = 10.0\n",
            "Evaluation Episode 97: Reward = 13.0\n",
            "Evaluation Episode 98: Reward = 18.0\n",
            "Evaluation Episode 99: Reward = 24.0\n",
            "Evaluation Episode 100: Reward = 9.0\n",
            "\n",
            "Mean Evaluation Reward over 100 episodes: 20.51\n"
          ]
        }
      ]
    }
  ]
}