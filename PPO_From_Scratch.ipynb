{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNnQQ0hW7GQBG7KrL+jPWe6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "# !pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR-QA8eyl9_Z",
        "outputId": "40aac45d-20d6-4b5c-ebf2-18594b2a0474"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "FdjPZTbIRfDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VBNaB4Ng0tis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b44a402b-ba51-4ebe-e6e1-3cf711727bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/vector/vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dones: tensor(18.)\n",
            "Policy 1 Mean Loss: 0.027829166094306856\n",
            "Entropy 1 Mean Loss: 0.6879273951053619\n",
            "Value 1 Mean Loss: 334.0935592651367\n",
            "dones: tensor(20.)\n",
            "Policy 2 Mean Loss: 0.023405245621688664\n",
            "Entropy 2 Mean Loss: 0.68889045342803\n",
            "Value 2 Mean Loss: 262.9704990386963\n",
            "dones: tensor(23.)\n",
            "Policy 3 Mean Loss: 0.010084584064316005\n",
            "Entropy 3 Mean Loss: 0.6909255422651768\n",
            "Value 3 Mean Loss: 212.22685146331787\n",
            "dones: tensor(16.)\n",
            "Policy 4 Mean Loss: 0.0050604111747816205\n",
            "Entropy 4 Mean Loss: 0.6919216588139534\n",
            "Value 4 Mean Loss: 419.1795234680176\n",
            "dones: tensor(21.)\n",
            "Policy 5 Mean Loss: 0.005738219537306577\n",
            "Entropy 5 Mean Loss: 0.6921195089817047\n",
            "Value 5 Mean Loss: 197.71795654296875\n",
            "dones: tensor(17.)\n",
            "Policy 6 Mean Loss: 0.009192784898914397\n",
            "Entropy 6 Mean Loss: 0.6922295726835728\n",
            "Value 6 Mean Loss: 294.1512641906738\n",
            "dones: tensor(25.)\n",
            "Policy 7 Mean Loss: 0.0026703894836828113\n",
            "Entropy 7 Mean Loss: 0.6925571784377098\n",
            "Value 7 Mean Loss: 165.35010528564453\n",
            "dones: tensor(23.)\n",
            "Policy 8 Mean Loss: 0.005550666130147874\n",
            "Entropy 8 Mean Loss: 0.692175105214119\n",
            "Value 8 Mean Loss: 205.77657508850098\n",
            "dones: tensor(20.)\n",
            "Policy 9 Mean Loss: 0.002273285237606615\n",
            "Entropy 9 Mean Loss: 0.6925733685493469\n",
            "Value 9 Mean Loss: 278.4694299697876\n",
            "dones: tensor(18.)\n",
            "Policy 10 Mean Loss: 0.004955428303219378\n",
            "Entropy 10 Mean Loss: 0.6926453821361065\n",
            "Value 10 Mean Loss: 226.95405387878418\n",
            "dones: tensor(20.)\n",
            "Policy 11 Mean Loss: -0.0005537931574508548\n",
            "Entropy 11 Mean Loss: 0.6925808526575565\n",
            "Value 11 Mean Loss: 237.76775932312012\n",
            "dones: tensor(25.)\n",
            "Policy 12 Mean Loss: 0.001522656879387796\n",
            "Entropy 12 Mean Loss: 0.6928590834140778\n",
            "Value 12 Mean Loss: 132.8260679244995\n",
            "dones: tensor(14.)\n",
            "Policy 13 Mean Loss: 0.0010914391023106873\n",
            "Entropy 13 Mean Loss: 0.6923960447311401\n",
            "Value 13 Mean Loss: 373.0320129394531\n",
            "dones: tensor(25.)\n",
            "Policy 14 Mean Loss: 0.002362756698857993\n",
            "Entropy 14 Mean Loss: 0.6925962530076504\n",
            "Value 14 Mean Loss: 169.21153831481934\n",
            "dones: tensor(20.)\n",
            "Policy 15 Mean Loss: 0.0024461409193463624\n",
            "Entropy 15 Mean Loss: 0.692435946315527\n",
            "Value 15 Mean Loss: 262.3305540084839\n",
            "dones: tensor(20.)\n",
            "Policy 16 Mean Loss: 0.004464819910936058\n",
            "Entropy 16 Mean Loss: 0.6926378905773163\n",
            "Value 16 Mean Loss: 183.2322826385498\n",
            "dones: tensor(18.)\n",
            "Policy 17 Mean Loss: 0.0008609055657871068\n",
            "Entropy 17 Mean Loss: 0.6926956325769424\n",
            "Value 17 Mean Loss: 234.1972303390503\n",
            "dones: tensor(21.)\n",
            "Policy 18 Mean Loss: 0.0029158471152186394\n",
            "Entropy 18 Mean Loss: 0.692508801817894\n",
            "Value 18 Mean Loss: 188.18335723876953\n",
            "dones: tensor(20.)\n",
            "Policy 19 Mean Loss: 0.0010091692383866757\n",
            "Entropy 19 Mean Loss: 0.6923451833426952\n",
            "Value 19 Mean Loss: 284.44364166259766\n",
            "dones: tensor(22.)\n",
            "Policy 20 Mean Loss: 0.0018950642552226782\n",
            "Entropy 20 Mean Loss: 0.6925690174102783\n",
            "Value 20 Mean Loss: 173.7999505996704\n",
            "dones: tensor(27.)\n",
            "Policy 21 Mean Loss: -0.003622354182880372\n",
            "Entropy 21 Mean Loss: 0.6926823854446411\n",
            "Value 21 Mean Loss: 112.29824686050415\n",
            "dones: tensor(17.)\n",
            "Policy 22 Mean Loss: 0.000607753696385771\n",
            "Entropy 22 Mean Loss: 0.692755751311779\n",
            "Value 22 Mean Loss: 251.66390132904053\n",
            "dones: tensor(23.)\n",
            "Policy 23 Mean Loss: 0.0019744463497772813\n",
            "Entropy 23 Mean Loss: 0.6928548477590084\n",
            "Value 23 Mean Loss: 163.0585265159607\n",
            "dones: tensor(20.)\n",
            "Policy 24 Mean Loss: 0.0020682626345660537\n",
            "Entropy 24 Mean Loss: 0.6925517097115517\n",
            "Value 24 Mean Loss: 232.40513134002686\n",
            "dones: tensor(22.)\n",
            "Policy 25 Mean Loss: 0.0009959342423826456\n",
            "Entropy 25 Mean Loss: 0.6926002241671085\n",
            "Value 25 Mean Loss: 208.6590747833252\n",
            "dones: tensor(21.)\n",
            "Policy 26 Mean Loss: -0.002398926531895995\n",
            "Entropy 26 Mean Loss: 0.6924806907773018\n",
            "Value 26 Mean Loss: 204.3023452758789\n",
            "dones: tensor(22.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-4178996120.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# get new logprobs(but don't overwrite), values, and entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mmb_new_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_new_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_new_values_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# note: may need to be flattened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# value optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-33-4178996120.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mvalue_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mmin_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mp_log_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mp_log_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"CartPole-v1\" # going to use \"CartPole-v1\"\n",
        "num_envs = 4\n",
        "rollouts = 2000\n",
        "middle_layer_size = 64 # Define the size of the middle layer\n",
        "batch_size = 128 # Define a batch size for reshaping\n",
        "num_epochs = 4\n",
        "num_mini_batches = 4\n",
        "clipping_coef = 0.2\n",
        "\n",
        "gamma = 0.99\n",
        "\n",
        "# Conventional Vectorized Environment wrapper\n",
        "def make_env(env_id, seed=None): # Added seed parameter\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        if seed is not None: # Set seed if provided\n",
        "            env.seed(seed)\n",
        "            env.action_space.seed(seed)\n",
        "            env.observation_space.seed(seed)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, observation_space_shape, action_space_size, middle_layer_size) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      # Actor/Policy\n",
        "      self.actor = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, action_space_size),\n",
        "          ) # Added dim=-1 to softmax\n",
        "\n",
        "      # Critic/Advantage NN //might need another activation function at the end.\n",
        "      self.critic = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, 1), # Output size of 1 for the value function\n",
        "          )\n",
        "\n",
        "\n",
        "  def predict(self, x, deterministic=False):\n",
        "    action_logits = self.actor(x)\n",
        "    if deterministic:\n",
        "        action = torch.argmax(action_logits, dim=-1)\n",
        "        log_prob = None  # Log probability is not well-defined for argmax\n",
        "        entropy = None   # Entropy is not well-defined for argmax\n",
        "    else:\n",
        "        act_dist = Categorical(logits=action_logits)\n",
        "        action = act_dist.sample()\n",
        "        log_prob = act_dist.log_prob(action)\n",
        "        entropy = act_dist.entropy() # Calculate entropy\n",
        "\n",
        "    value_logits = self.critic(x)\n",
        "\n",
        "    # return entropy, probabilies, and sampled action\n",
        "    return (entropy, log_prob, action, value_logits) # Return entropy, probabilities, and a sampled action\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Corrected __main__\n",
        "\n",
        "  # initilizattion\n",
        "  envs = gym.vector.AsyncVectorEnv([make_env(env_id, seed=(i**2)) for i in range(num_envs)]) # Pass individual seeds\n",
        "\n",
        "  # Get observation and action space dimensions\n",
        "  observation_space_shape = envs.single_observation_space.shape[0] # Assuming flat observation space\n",
        "  action_space_size = envs.single_action_space.n # Assuming discrete action space\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #initialize the Agent\n",
        "  agent = Agent(observation_space_shape, action_space_size, middle_layer_size).to(device) # Pass dimensions and move to device\n",
        "\n",
        "  #initialize the optimizer\n",
        "  optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
        "\n",
        "  # Initialize tensors with appropriate shapes\n",
        "  obs = torch.zeros((batch_size, num_envs, observation_space_shape)).to(device)\n",
        "  actions = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  logprobs = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  rewards = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  dones = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  pred_values = torch.zeros((batch_size, num_envs)).to(device)\n",
        "\n",
        "  # init actual values and advantages tensor\n",
        "  actual_values = torch.zeros_like(rewards).to(device)\n",
        "  advantages = torch.zeros_like(rewards).to(device)\n",
        "\n",
        "\n",
        "  # initializes the observation, done, the time, and the step\n",
        "  start_time = time.time()\n",
        "  global_step = 0\n",
        "\n",
        "  # define training regime\n",
        "  for i in range(int(rollouts)): # Cast steps to int\n",
        "    # Learning rate annealing\n",
        "    frac = 1.0 - (i / rollouts)\n",
        "    lr_now = 2.5e-4 * frac # Anneal from initial learning rate\n",
        "    optimizer.param_groups[0]['lr'] = lr_now\n",
        "\n",
        "\n",
        "    step = 0 # Initialize step counter for batch\n",
        "    next_obs = torch.Tensor(envs.reset()).to(device) # Corrected envs.reset()\n",
        "    next_done =  torch.zeros((num_envs,)).to(device)\n",
        "\n",
        "    for step in range(int(batch_size)):\n",
        "      # get actions, observations, rewards, and dones\n",
        "      with torch.no_grad(): # Added no_grad for inference\n",
        "          _, log_prob, action, values_ = agent.predict(next_obs) # Renamed values to values_ to avoid conflict\n",
        "\n",
        "      # Move data to tensors\n",
        "      next_obs_np, rewards_np, next_done_np, infos =  envs.step(action.cpu().numpy()) # env step and move action to cpu\n",
        "\n",
        "      # Moves things that were on the cpu onto the gpu\n",
        "      next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "      next_done = torch.Tensor(next_done_np).to(device)\n",
        "      reward = torch.Tensor(rewards_np).to(device)\n",
        "\n",
        "      # Store data in tensors at the current step\n",
        "      obs[step] = next_obs\n",
        "      actions[step] = action\n",
        "      logprobs[step] = log_prob.detach()\n",
        "      rewards[step] = reward\n",
        "      dones[step] = next_done\n",
        "      pred_values[step] = values_.squeeze(-1).detach() # Remove the last dimension of size 1\n",
        "\n",
        "      global_step += num_envs # Update global step\n",
        "\n",
        "    # calculate actual values at each time step'\n",
        "    print(\"dones:\", dones.sum().cpu())\n",
        "\n",
        "    with torch.no_grad(): # Calculate advantages outside the gradient tape\n",
        "        for t in reversed(range(batch_size)):\n",
        "          if t == batch_size - 1:\n",
        "              # For the last step, if the environment is not done, use the value of the next state (from the agent's prediction)\n",
        "              # Otherwise, the actual value is just the reward at this step\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              next_value = agent.critic(next_obs).squeeze(-1).detach() # bootstrap next value since it doesn't exsist\n",
        "          else:\n",
        "              # For other steps, if the environment at the next step is not done, use the value of the next state from the stored values\n",
        "              # Otherwise, the actual value is just the reward at this step\n",
        "              nextnonterminal = 1.0 - dones[t+1]\n",
        "              next_value = actual_values[t+1]\n",
        "          actual_values[t] = rewards[t] + gamma * next_value * nextnonterminal\n",
        "        advantages = actual_values - pred_values.detach() # Detach pred_values here\n",
        "\n",
        "\n",
        "    # Actually training the agent neural net\n",
        "\n",
        "    # flattening the tensors for ease\n",
        "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "    b_logprobs = logprobs.reshape(-1)\n",
        "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "    b_advantages = advantages.reshape(-1)\n",
        "    b_actual_values = actual_values.reshape(-1)\n",
        "    b_pred_values = pred_values.reshape(-1)\n",
        "\n",
        "    # creates storage to see loss over time\n",
        "    Policy_loss_array = [] # Initialize list to store losses for the current rollout\n",
        "    Entropy_loss_array = [] # Initialize list to store losses for the current rollout\n",
        "    Value_loss_array = [] # Initialize list to store losses for the current rollout\n",
        "\n",
        "    # Iterates over the same batch a couple times for efficiency\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      #seperates into minibatches\n",
        "      indices = np.arange(batch_size * num_envs)   # creates indicies\n",
        "      np.random.shuffle(indices)        # shuffles indicies\n",
        "      minibatch_indices = np.array_split(indices, num_mini_batches)\n",
        "\n",
        "      #iterates over the minibatches\n",
        "      for current_minibatch_indices in minibatch_indices: # Corrected indexing for minibatch_indices\n",
        "\n",
        "        # init mini_batch\n",
        "        mb_obs = b_obs[current_minibatch_indices]\n",
        "        mb_log_probs = b_logprobs[current_minibatch_indices]\n",
        "        mb_actions = b_actions[current_minibatch_indices] # Corrected indexing for mb_actions\n",
        "        mb_advantages = b_advantages[current_minibatch_indices]\n",
        "        mb_actual_values = b_actual_values[current_minibatch_indices]\n",
        "        mb_pred_values = b_pred_values[current_minibatch_indices].detach() # Detach old predicted values for clipping\n",
        "\n",
        "\n",
        "        # get new logprobs(but don't overwrite), values, and entropy\n",
        "        mb_new_entropy, mb_new_log_probs, _, mb_new_values_ = agent.predict(mb_obs) # note: may need to be flattened\n",
        "\n",
        "        # value optimization\n",
        "        unclipped_value_loss = (mb_actual_values - mb_new_values_.squeeze(-1)) ** 2\n",
        "\n",
        "        clipped_predicted_values = mb_pred_values + torch.clamp(mb_new_values_.squeeze()- mb_pred_values, -clipping_coef, clipping_coef)\n",
        "        clipped_value_Loss = (mb_actual_values - clipped_predicted_values ) ** 2\n",
        "\n",
        "        value_loss = torch.max(unclipped_value_loss, clipped_value_Loss).mean()\n",
        "\n",
        "        # policy optimization\n",
        "\n",
        "        # Normalize advantages\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8) # Add a small epsilon for numerical stability\n",
        "\n",
        "        # calculate ratios\n",
        "        unclipped_ratio = (mb_new_log_probs - mb_log_probs).exp()\n",
        "        clipped_ratio = torch.clamp(unclipped_ratio, 1 - clipping_coef, 1 + clipping_coef)\n",
        "\n",
        "        # calculate loss\n",
        "        policy_loss = torch.max(-mb_advantages*unclipped_ratio, -mb_advantages*clipped_ratio).mean()\n",
        "\n",
        "        # calculates entropy\n",
        "        entropy_loss = mb_new_entropy.mean()\n",
        "\n",
        "        #calculates total loss\n",
        "\n",
        "        loss = (policy_loss*2) - (entropy_loss*0.01) + (value_loss * 0.15)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        Policy_loss_array.append(policy_loss.item()) # Append mini-batch loss to list\n",
        "        Entropy_loss_array.append(entropy_loss.item()) # Append mini-batch loss to list\n",
        "        Value_loss_array.append(value_loss.item()) # Append mini-batch loss to list\n",
        "\n",
        "    # Calculate and print mean loss for the current rollout\n",
        "    mean_policy_loss = np.mean(Policy_loss_array)\n",
        "    mean_entropy_loss = np.mean(Entropy_loss_array)\n",
        "    mean_value_loss = np.mean(Value_loss_array)\n",
        "\n",
        "    print(f\"Policy {i+1} Mean Loss: {mean_policy_loss}\")\n",
        "    print(f\"Entropy {i+1} Mean Loss: {mean_entropy_loss}\")\n",
        "    print(f\"Value {i+1} Mean Loss: {mean_value_loss}\")\n",
        "\n",
        "\n",
        "  envs.close() # Close the environment\n",
        "\n",
        "  # Save the agent's state dictionary\n",
        "  torch.save(agent.state_dict(), \"agent.pth\")\n",
        "  print(\"Agent state saved to agent.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo2bN-hrJ5yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc48b610",
        "outputId": "ddd45126-c420-4fef-f339-7db27d38d3cf"
      },
      "source": [
        "# Set up evaluation environment\n",
        "eval_env = gym.make(env_id, render_mode='rgb_array') # Use render_mode for video recording\n",
        "\n",
        "# Optional: Wrap the environment to record video\n",
        "# You might need to install 'moviepy' and 'ffmpeg' for this.\n",
        "# !pip install moviepy ffmpeg\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "# Create a directory to save videos\n",
        "video_folder = \"./eval_videos\"\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "eval_env = RecordVideo(eval_env, video_folder)\n",
        "\n",
        "# Load the trained agent's state (assuming agent is still in memory or saved)\n",
        "# If you saved the agent, you would load it here:\n",
        "try:\n",
        "    agent.load_state_dict(torch.load(\"agent.pth\"))\n",
        "    print(\"Agent state loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Agent state file not found. Please run the training cell first.\")\n",
        "    # Optionally, handle this case by exiting or using an untrained agent\n",
        "    # For now, we'll continue with the current agent instance (likely untrained if file not found)\n",
        "\n",
        "\n",
        "# Set agent to evaluation mode\n",
        "agent.eval()\n",
        "\n",
        "# Run evaluation episodes\n",
        "num_eval_episodes = 10 # Number of episodes for evaluation\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_eval_episodes):\n",
        "    obs = eval_env.reset() # Correctly unpack observation and info\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        with torch.no_grad(): # Use no_grad for inference\n",
        "            # Convert observation to tensor and move to device\n",
        "            obs_tensor = torch.Tensor(obs).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "            # Get action from the agent (use predict for single environment inference)\n",
        "            _, _, action, _ = agent.predict(obs_tensor)\n",
        "\n",
        "            # Remove batch dimension and move action to cpu for environment step\n",
        "            # For a single discrete action, get the scalar value\n",
        "            action_np = action.squeeze(0).cpu().numpy().item()\n",
        "\n",
        "\n",
        "        # Step the environment\n",
        "        obs, reward, terminated, truncated = eval_env.step(action_np) # Correctly unpack all 5 values\n",
        "        episode_reward += reward\n",
        "        done = terminated # Consider either terminated or truncated as done for episode termination\n",
        "\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward}\")\n",
        "\n",
        "# Close the evaluation environment\n",
        "# Attempt to close the underlying environment directly as a workaround for potential wrapper close issues\n",
        "eval_env.env.close()\n",
        "\n",
        "\n",
        "# Calculate and print mean reward\n",
        "mean_eval_reward = np.mean(episode_rewards)\n",
        "print(f\"\\nMean Evaluation Reward over {num_eval_episodes} episodes: {mean_eval_reward}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/eval_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent state loaded successfully.\n",
            "Evaluation Episode 1: Reward = 19.0\n",
            "Evaluation Episode 2: Reward = 40.0\n",
            "Evaluation Episode 3: Reward = 11.0\n",
            "Evaluation Episode 4: Reward = 16.0\n",
            "Evaluation Episode 5: Reward = 15.0\n",
            "Evaluation Episode 6: Reward = 30.0\n",
            "Evaluation Episode 7: Reward = 11.0\n",
            "Evaluation Episode 8: Reward = 63.0\n",
            "Evaluation Episode 9: Reward = 17.0\n",
            "Evaluation Episode 10: Reward = 27.0\n",
            "\n",
            "Mean Evaluation Reward over 10 episodes: 24.9\n"
          ]
        }
      ]
    }
  ]
}