{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNGQqQwSCIY451UT9buV+cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "# !pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "hR-QA8eyl9_Z",
        "outputId": "fc2e1776-732e-42f3-870c-dd60973628a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.24.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.11.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "geopandas 1.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scipy 1.16.0 requires numpy<2.6,>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "a9cc3576c31c4e77a599f32e98cb095b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "FdjPZTbIRfDK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VBNaB4Ng0tis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc2f5b6-4763-435b-8116-f0b3a89e330b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/vector/vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5352)\n",
            "Rollout 1 Mean Loss: 24.013316750526428\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5273)\n",
            "Rollout 2 Mean Loss: 23.661728978157043\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 3 Mean Loss: 25.412558674812317\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.4990)\n",
            "Rollout 4 Mean Loss: 24.14843249320984\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5059)\n",
            "Rollout 5 Mean Loss: 22.400730848312378\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5322)\n",
            "Rollout 6 Mean Loss: 19.195423126220703\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.4971)\n",
            "Rollout 7 Mean Loss: 19.45271098613739\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5342)\n",
            "Rollout 8 Mean Loss: 20.69362187385559\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5430)\n",
            "Rollout 9 Mean Loss: 22.09918737411499\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5049)\n",
            "Rollout 10 Mean Loss: 21.21896731853485\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5020)\n",
            "Rollout 11 Mean Loss: 19.791807889938354\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5156)\n",
            "Rollout 12 Mean Loss: 19.084627389907837\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5264)\n",
            "Rollout 13 Mean Loss: 19.375482439994812\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5322)\n",
            "Rollout 14 Mean Loss: 18.432628750801086\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5439)\n",
            "Rollout 15 Mean Loss: 20.65386390686035\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5508)\n",
            "Rollout 16 Mean Loss: 19.256980538368225\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5469)\n",
            "Rollout 17 Mean Loss: 16.17341583967209\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5117)\n",
            "Rollout 18 Mean Loss: 20.856911182403564\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5391)\n",
            "Rollout 19 Mean Loss: 18.7826589345932\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5156)\n",
            "Rollout 20 Mean Loss: 17.527508676052094\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5391)\n",
            "Rollout 21 Mean Loss: 16.782581746578217\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 22 Mean Loss: 16.395040273666382\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5469)\n",
            "Rollout 23 Mean Loss: 15.601723432540894\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5352)\n",
            "Rollout 24 Mean Loss: 15.530759334564209\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5137)\n",
            "Rollout 25 Mean Loss: 14.790473937988281\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5225)\n",
            "Rollout 26 Mean Loss: 15.377377390861511\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5566)\n",
            "Rollout 27 Mean Loss: 13.547739803791046\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5312)\n",
            "Rollout 28 Mean Loss: 16.873397171497345\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5518)\n",
            "Rollout 29 Mean Loss: 12.297002971172333\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5332)\n",
            "Rollout 30 Mean Loss: 14.992042183876038\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5498)\n",
            "Rollout 31 Mean Loss: 13.948505818843842\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5068)\n",
            "Rollout 32 Mean Loss: 14.504920601844788\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5176)\n",
            "Rollout 33 Mean Loss: 14.305619537830353\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5654)\n",
            "Rollout 34 Mean Loss: 11.626573741436005\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5449)\n",
            "Rollout 35 Mean Loss: 13.700744807720184\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5352)\n",
            "Rollout 36 Mean Loss: 13.259638965129852\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5332)\n",
            "Rollout 37 Mean Loss: 11.524173200130463\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5127)\n",
            "Rollout 38 Mean Loss: 13.046911537647247\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5322)\n",
            "Rollout 39 Mean Loss: 12.226528882980347\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5361)\n",
            "Rollout 40 Mean Loss: 13.14892452955246\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5605)\n",
            "Rollout 41 Mean Loss: 10.682145178318024\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5605)\n",
            "Rollout 42 Mean Loss: 11.40950208902359\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5293)\n",
            "Rollout 43 Mean Loss: 11.484704613685608\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5146)\n",
            "Rollout 44 Mean Loss: 10.54721862077713\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5127)\n",
            "Rollout 45 Mean Loss: 9.526138544082642\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5098)\n",
            "Rollout 46 Mean Loss: 10.087727308273315\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5459)\n",
            "Rollout 47 Mean Loss: 10.642972230911255\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5420)\n",
            "Rollout 48 Mean Loss: 9.832964658737183\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5469)\n",
            "Rollout 49 Mean Loss: 9.278149545192719\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5195)\n",
            "Rollout 50 Mean Loss: 10.071676254272461\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5273)\n",
            "Rollout 51 Mean Loss: 10.666529178619385\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5244)\n",
            "Rollout 52 Mean Loss: 10.10620665550232\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5039)\n",
            "Rollout 53 Mean Loss: 10.462855756282806\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5352)\n",
            "Rollout 54 Mean Loss: 9.318746864795685\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5293)\n",
            "Rollout 55 Mean Loss: 8.759438693523407\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5322)\n",
            "Rollout 56 Mean Loss: 8.761775493621826\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5283)\n",
            "Rollout 57 Mean Loss: 9.41021341085434\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5488)\n",
            "Rollout 58 Mean Loss: 7.542315483093262\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5225)\n",
            "Rollout 59 Mean Loss: 7.426159530878067\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5234)\n",
            "Rollout 60 Mean Loss: 6.7793726325035095\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5381)\n",
            "Rollout 61 Mean Loss: 9.132396429777145\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 62 Mean Loss: 8.83296149969101\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5303)\n",
            "Rollout 63 Mean Loss: 7.005038470029831\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5215)\n",
            "Rollout 64 Mean Loss: 8.149369925260544\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5029)\n",
            "Rollout 65 Mean Loss: 7.777435630559921\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5732)\n",
            "Rollout 66 Mean Loss: 6.1494830548763275\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5146)\n",
            "Rollout 67 Mean Loss: 7.721244841814041\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5137)\n",
            "Rollout 68 Mean Loss: 6.550571709871292\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5254)\n",
            "Rollout 69 Mean Loss: 7.871128350496292\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5439)\n",
            "Rollout 70 Mean Loss: 6.753582239151001\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5293)\n",
            "Rollout 71 Mean Loss: 7.06389394402504\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 72 Mean Loss: 6.0377190709114075\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5049)\n",
            "Rollout 73 Mean Loss: 7.110762000083923\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5332)\n",
            "Rollout 74 Mean Loss: 5.97113561630249\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.4961)\n",
            "Rollout 75 Mean Loss: 7.1691708862781525\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5186)\n",
            "Rollout 76 Mean Loss: 6.7753986120224\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5508)\n",
            "Rollout 77 Mean Loss: 6.45957088470459\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5508)\n",
            "Rollout 78 Mean Loss: 5.865075707435608\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5068)\n",
            "Rollout 79 Mean Loss: 6.167021036148071\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5518)\n",
            "Rollout 80 Mean Loss: 5.655716627836227\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5625)\n",
            "Rollout 81 Mean Loss: 7.055557668209076\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5537)\n",
            "Rollout 82 Mean Loss: 6.023317933082581\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5439)\n",
            "Rollout 83 Mean Loss: 6.978320986032486\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5527)\n",
            "Rollout 84 Mean Loss: 6.452750086784363\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5283)\n",
            "Rollout 85 Mean Loss: 6.528528958559036\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5215)\n",
            "Rollout 86 Mean Loss: 6.725679963827133\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5195)\n",
            "Rollout 87 Mean Loss: 5.636937886476517\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5322)\n",
            "Rollout 88 Mean Loss: 5.668644517660141\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5420)\n",
            "Rollout 89 Mean Loss: 5.850981771945953\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5137)\n",
            "Rollout 90 Mean Loss: 5.664060533046722\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 91 Mean Loss: 5.961169511079788\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 92 Mean Loss: 6.562670648097992\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5400)\n",
            "Rollout 93 Mean Loss: 5.1360123455524445\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5498)\n",
            "Rollout 94 Mean Loss: 5.700656086206436\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5420)\n",
            "Rollout 95 Mean Loss: 5.529020309448242\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5625)\n",
            "Rollout 96 Mean Loss: 5.533774137496948\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5352)\n",
            "Rollout 97 Mean Loss: 6.238586604595184\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5234)\n",
            "Rollout 98 Mean Loss: 5.881429731845856\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.5205)\n",
            "Rollout 99 Mean Loss: 6.401173144578934\n",
            "mean reward:  tensor(1024.)\n",
            "mean action:  tensor(0.4961)\n",
            "Rollout 100 Mean Loss: 6.345510303974152\n",
            "Agent state saved to agent.pth\n",
            "Agent state loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/eval_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Episode 1: Reward = 20.0\n",
            "Evaluation Episode 2: Reward = 15.0\n",
            "Evaluation Episode 3: Reward = 13.0\n",
            "Evaluation Episode 4: Reward = 11.0\n",
            "Evaluation Episode 5: Reward = 14.0\n",
            "Evaluation Episode 6: Reward = 11.0\n",
            "Evaluation Episode 7: Reward = 39.0\n",
            "Evaluation Episode 8: Reward = 18.0\n",
            "Evaluation Episode 9: Reward = 17.0\n",
            "Evaluation Episode 10: Reward = 25.0\n",
            "\n",
            "Mean Evaluation Reward over 10 episodes: 18.3\n"
          ]
        }
      ],
      "source": [
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"CartPole-v1\" # going to use \"CartPole-v1\"\n",
        "clipping_coef = 0.1\n",
        "num_envs = 4\n",
        "rollouts = 100\n",
        "middle_layer_size = 64 # Define the size of the middle layer\n",
        "batch_size = 256 # Define a batch size for reshaping\n",
        "num_epochs = 4\n",
        "num_mini_batches = 4\n",
        "clipping_coef = 0.2\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "# Conventional Vectorized Environment wrapper\n",
        "def make_env(env_id, seed=None): # Added seed parameter\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        if seed is not None: # Set seed if provided\n",
        "            env.seed(seed)\n",
        "            env.action_space.seed(seed)\n",
        "            env.observation_space.seed(seed)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, observation_space_shape, action_space_size, middle_layer_size) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      # Actor/Policy\n",
        "      self.actor = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, action_space_size),\n",
        "          nn.Softmax(dim=-1)\n",
        "          ) # Added dim=-1 to softmax\n",
        "\n",
        "      # Critic/Advantage NN //might need another activation function at the end.\n",
        "      self.critic = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, 1), # Output size of 1 for the value function\n",
        "          )\n",
        "\n",
        "  def predict(self, x):\n",
        "    action_probs = self.actor(x)\n",
        "    act_dist = Categorical(action_probs)\n",
        "    action = act_dist.sample()\n",
        "    log_prob = act_dist.log_prob(action)\n",
        "    entropy = act_dist.entropy() # Calculate entropy\n",
        "\n",
        "    value_logits = self.critic(x)\n",
        "\n",
        "    # return entropy, probabilies, and sampled action\n",
        "    return (entropy, log_prob, action, value_logits) # Return entropy, probabilities, and a sampled action\n",
        "\n",
        "  def evaluate(self, x, action):\n",
        "      action_probs = self.actor(x)\n",
        "      dist = Categorical(action_probs)\n",
        "      action_logprobs = dist.log_prob(action)\n",
        "      dist_entropy = dist.entropy()\n",
        "      state_values = self.critic(x)\n",
        "\n",
        "      return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Corrected __main__\n",
        "\n",
        "  # initilizattion\n",
        "  envs = gym.vector.AsyncVectorEnv([make_env(env_id, seed=(i**2)) for i in range(num_envs)]) # Pass individual seeds\n",
        "\n",
        "  # Get observation and action space dimensions\n",
        "  observation_space_shape = envs.single_observation_space.shape[0] # Assuming flat observation space\n",
        "  action_space_size = envs.single_action_space.n # Assuming discrete action space\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #initialize the Agent\n",
        "  agent = Agent(observation_space_shape, action_space_size, middle_layer_size).to(device) # Pass dimensions and move to device\n",
        "\n",
        "  #initialize the optimizer\n",
        "  optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
        "\n",
        "  # Initialize tensors with appropriate shapes\n",
        "  obs = torch.zeros((batch_size, num_envs, observation_space_shape)).to(device)\n",
        "  actions = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  logprobs = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  rewards = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  dones = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  pred_values = torch.zeros((batch_size, num_envs)).to(device)\n",
        "\n",
        "  # init actual values and advantages tensor\n",
        "  actual_values = torch.zeros_like(rewards).to(device)\n",
        "  advantages = torch.zeros_like(rewards).to(device)\n",
        "\n",
        "\n",
        "  # initializes the observation, done, the time, and the step\n",
        "  start_time = time.time()\n",
        "  global_step = 0\n",
        "\n",
        "  # define training regime\n",
        "  for i in range(int(rollouts)): # Cast steps to int\n",
        "    # Learning rate annealing\n",
        "    frac = 1.0 - (i / rollouts)\n",
        "    lr_now = 2.5e-4 * frac # Anneal from initial learning rate\n",
        "    optimizer.param_groups[0]['lr'] = lr_now\n",
        "\n",
        "\n",
        "    step = 0 # Initialize step counter for batch\n",
        "    next_obs = torch.Tensor(envs.reset()).to(device) # Corrected envs.reset()\n",
        "    next_done =  torch.zeros((num_envs,)).to(device)\n",
        "\n",
        "    for step in range(int(batch_size)):\n",
        "      # get actions, observations, rewards, and dones\n",
        "      with torch.no_grad(): # Added no_grad for inference\n",
        "          _, log_prob, action, values_ = agent.predict(next_obs) # Renamed values to values_ to avoid conflict\n",
        "\n",
        "      # Move data to tensors\n",
        "      next_obs_np, rewards_np, next_done_np, infos =  envs.step(action.cpu().numpy()) # env step and move action to cpu\n",
        "\n",
        "      # Moves things that were on the cpu onto the gpu\n",
        "      next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "      next_done = torch.Tensor(next_done_np).to(device)\n",
        "      reward = torch.Tensor(rewards_np).to(device)\n",
        "\n",
        "      # Store data in tensors at the current step\n",
        "      obs[step] = next_obs\n",
        "      actions[step] = action\n",
        "      logprobs[step] = log_prob.detach()\n",
        "      rewards[step] = reward\n",
        "      dones[step] = next_done\n",
        "      pred_values[step] = values_.squeeze(-1).detach() # Remove the last dimension of size 1\n",
        "\n",
        "      global_step += num_envs # Update global step\n",
        "\n",
        "    # calculate actual values at each time step'\n",
        "    print(\"mean reward: \", rewards.sum().cpu())\n",
        "    print(\"mean action: \", actions.mean().cpu())\n",
        "\n",
        "    with torch.no_grad(): # Calculate advantages outside the gradient tape\n",
        "        for t in reversed(range(batch_size)):\n",
        "          if t == batch_size - 1:\n",
        "              # For the last step, if the environment is not done, use the value of the next state (from the agent's prediction)\n",
        "              # Otherwise, the actual value is just the reward at this step\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              next_value = agent.critic(next_obs).squeeze(-1).detach() # bootstrap next value since it doesn't exsist\n",
        "          else:\n",
        "              # For other steps, if the environment at the next step is not done, use the value of the next state from the stored values\n",
        "              # Otherwise, the actual value is just the reward at this step\n",
        "              nextnonterminal = 1.0 - dones[t+1]\n",
        "              next_value = actual_values[t+1]\n",
        "          actual_values[t] = rewards[t] + gamma * next_value * nextnonterminal\n",
        "        advantages = actual_values - pred_values.detach() # Detach pred_values here\n",
        "\n",
        "\n",
        "    # Actually training the agent neural net\n",
        "\n",
        "    # flattening the tensors for ease\n",
        "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "    b_logprobs = logprobs.reshape(-1)\n",
        "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "    b_advantages = advantages.reshape(-1)\n",
        "    b_actual_values = actual_values.reshape(-1)\n",
        "    b_pred_values = pred_values.reshape(-1)\n",
        "\n",
        "    # creates storage to see loss over time\n",
        "    rollout_losses = [] # Initialize list to store losses for the current rollout\n",
        "\n",
        "    # Iterates over the same batch a couple times for efficiency\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      #seperates into minibatches\n",
        "      indices = np.arange(batch_size * num_envs)   # creates indicies\n",
        "      np.random.shuffle(indices)        # shuffles indicies\n",
        "      minibatch_indices = np.array_split(indices, num_mini_batches)\n",
        "\n",
        "      #iterates over the minibatches\n",
        "      for current_minibatch_indices in minibatch_indices: # Corrected indexing for minibatch_indices\n",
        "\n",
        "        # init mini_batch\n",
        "        mb_obs = b_obs[current_minibatch_indices]\n",
        "        mb_log_probs = b_logprobs[current_minibatch_indices]\n",
        "        mb_actions = b_actions[current_minibatch_indices] # Corrected indexing for mb_actions\n",
        "        mb_advantages = b_advantages[current_minibatch_indices]\n",
        "        mb_actual_values = b_actual_values[current_minibatch_indices]\n",
        "        mb_pred_values = b_pred_values[current_minibatch_indices].detach() # Detach old predicted values for clipping\n",
        "\n",
        "\n",
        "        # get new logprobs(but don't overwrite), values, and entropy\n",
        "        mb_new_entropy, mb_new_log_probs, _, mb_new_values_ = agent.predict(mb_obs) # note: may need to be flattened\n",
        "\n",
        "        # value optimization\n",
        "        unclipped_value_loss = (mb_actual_values - mb_new_values_.squeeze(-1)) ** 2\n",
        "\n",
        "        clipped_predicted_values = mb_pred_values + torch.clamp(mb_new_values_.squeeze()- mb_pred_values, -clipping_coef, clipping_coef)\n",
        "        clipped_value_Loss = (mb_actual_values - clipped_predicted_values ) ** 2\n",
        "\n",
        "        value_loss = torch.max(unclipped_value_loss, clipped_value_Loss).mean()\n",
        "\n",
        "        # policy optimization\n",
        "\n",
        "        # Normalize advantages\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8) # Add a small epsilon for numerical stability\n",
        "\n",
        "        # calculate ratios\n",
        "        unclipped_ratio = (mb_new_log_probs - mb_log_probs).exp()\n",
        "        clipped_ratio = torch.clamp(unclipped_ratio, 1 - clipping_coef, 1 + clipping_coef)\n",
        "\n",
        "        # calculate loss\n",
        "        policy_loss = torch.max(-mb_advantages*unclipped_ratio, -mb_advantages*clipped_ratio).mean()\n",
        "\n",
        "        # calculates entropy\n",
        "        entropy_loss = mb_new_entropy.mean()\n",
        "\n",
        "        #calculates total loss\n",
        "\n",
        "        loss = policy_loss - (entropy_loss*0.01) + (value_loss * 0.5)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        rollout_losses.append(loss.item()) # Append mini-batch loss to list\n",
        "\n",
        "    # Calculate and print mean loss for the current rollout\n",
        "    mean_rollout_loss = np.mean(rollout_losses)\n",
        "    print(f\"Rollout {i+1} Mean Loss: {mean_rollout_loss}\")\n",
        "\n",
        "\n",
        "  envs.close() # Close the environment\n",
        "\n",
        "  # Save the agent's state dictionary\n",
        "  torch.save(agent.state_dict(), \"agent.pth\")\n",
        "  print(\"Agent state saved to agent.pth\")\n",
        "\n",
        "\n",
        "  eval_env = gym.make(env_id, render_mode='rgb_array') # Use render_mode for video recording\n",
        "\n",
        "  # Optional: Wrap the environment to record video\n",
        "  # You might need to install 'moviepy' and 'ffmpeg' for this.\n",
        "  # !pip install moviepy ffmpeg\n",
        "  from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "  # Create a directory to save videos\n",
        "  video_folder = \"./eval_videos\"\n",
        "  os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "  eval_env = RecordVideo(eval_env, video_folder)\n",
        "\n",
        "  # Load the trained agent's state (assuming agent is still in memory or saved)\n",
        "  # If you saved the agent, you would load it here:\n",
        "  try:\n",
        "      agent.load_state_dict(torch.load(\"agent.pth\"))\n",
        "      print(\"Agent state loaded successfully.\")\n",
        "  except FileNotFoundError:\n",
        "      print(\"Agent state file not found. Please run the training cell first.\")\n",
        "      # Optionally, handle this case by exiting or using an untrained agent\n",
        "      # For now, we'll continue with the current agent instance (likely untrained if file not found)\n",
        "\n",
        "\n",
        "  # Set agent to evaluation mode\n",
        "  agent.eval()\n",
        "\n",
        "  # Run evaluation episodes\n",
        "  num_eval_episodes = 10 # Number of episodes for evaluation\n",
        "  episode_rewards = []\n",
        "\n",
        "  for episode in range(num_eval_episodes):\n",
        "      obs = eval_env.reset() # Correctly unpack observation and info\n",
        "      done = False\n",
        "      episode_reward = 0\n",
        "\n",
        "      while not done:\n",
        "          with torch.no_grad(): # Use no_grad for inference\n",
        "              # Convert observation to tensor and move to device\n",
        "              obs_tensor = torch.Tensor(obs).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "              # Get action from the agent (use predict for single environment inference)\n",
        "              _, _, action, _ = agent.predict(obs_tensor)\n",
        "\n",
        "              # Remove batch dimension and move action to cpu for environment step\n",
        "              # For a single discrete action, get the scalar value\n",
        "              action_np = action.squeeze(0).cpu().numpy().item()\n",
        "\n",
        "\n",
        "          # Step the environment\n",
        "          obs, reward, terminated, truncated = eval_env.step(action_np) # Correctly unpack all 5 values\n",
        "          episode_reward += reward\n",
        "          done = terminated # Consider either terminated or truncated as done for episode termination\n",
        "\n",
        "\n",
        "      episode_rewards.append(episode_reward)\n",
        "      print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward}\")\n",
        "\n",
        "  # Close the evaluation environment\n",
        "  # Attempt to close the underlying environment directly as a workaround for potential wrapper close issues\n",
        "  eval_env.env.close()\n",
        "\n",
        "\n",
        "  # Calculate and print mean reward\n",
        "  mean_eval_reward = np.mean(episode_rewards)\n",
        "  print(f\"\\nMean Evaluation Reward over {num_eval_episodes} episodes: {mean_eval_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo2bN-hrJ5yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc48b610",
        "outputId": "763a60d0-6f2b-4b5c-f632-9a2d26029ad7"
      },
      "source": [
        "# Set up evaluation environment\n",
        "eval_env = gym.make(env_id, render_mode='rgb_array') # Use render_mode for video recording\n",
        "\n",
        "# Optional: Wrap the environment to record video\n",
        "# You might need to install 'moviepy' and 'ffmpeg' for this.\n",
        "# !pip install moviepy ffmpeg\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "# Create a directory to save videos\n",
        "video_folder = \"./eval_videos\"\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "eval_env = RecordVideo(eval_env, video_folder)\n",
        "\n",
        "# Load the trained agent's state (assuming agent is still in memory or saved)\n",
        "# If you saved the agent, you would load it here:\n",
        "try:\n",
        "    agent.load_state_dict(torch.load(\"agent.pth\"))\n",
        "    print(\"Agent state loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Agent state file not found. Please run the training cell first.\")\n",
        "    # Optionally, handle this case by exiting or using an untrained agent\n",
        "    # For now, we'll continue with the current agent instance (likely untrained if file not found)\n",
        "\n",
        "\n",
        "# Set agent to evaluation mode\n",
        "agent.eval()\n",
        "\n",
        "# Run evaluation episodes\n",
        "num_eval_episodes = 10 # Number of episodes for evaluation\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_eval_episodes):\n",
        "    obs = eval_env.reset() # Correctly unpack observation and info\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        with torch.no_grad(): # Use no_grad for inference\n",
        "            # Convert observation to tensor and move to device\n",
        "            obs_tensor = torch.Tensor(obs).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "            # Get action from the agent (use predict for single environment inference)\n",
        "            _, _, action, _ = agent.predict(obs_tensor)\n",
        "\n",
        "            # Remove batch dimension and move action to cpu for environment step\n",
        "            # For a single discrete action, get the scalar value\n",
        "            action_np = action.squeeze(0).cpu().numpy().item()\n",
        "\n",
        "\n",
        "        # Step the environment\n",
        "        obs, reward, terminated, truncated = eval_env.step(action_np) # Correctly unpack all 5 values\n",
        "        episode_reward += reward\n",
        "        done = terminated # Consider either terminated or truncated as done for episode termination\n",
        "\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward}\")\n",
        "\n",
        "# Close the evaluation environment\n",
        "# Attempt to close the underlying environment directly as a workaround for potential wrapper close issues\n",
        "eval_env.env.close()\n",
        "\n",
        "\n",
        "# Calculate and print mean reward\n",
        "mean_eval_reward = np.mean(episode_rewards)\n",
        "print(f\"\\nMean Evaluation Reward over {num_eval_episodes} episodes: {mean_eval_reward}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/eval_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent state loaded successfully.\n",
            "Evaluation Episode 1: Reward = 10.0\n",
            "Evaluation Episode 2: Reward = 58.0\n",
            "Evaluation Episode 3: Reward = 19.0\n",
            "Evaluation Episode 4: Reward = 23.0\n",
            "Evaluation Episode 5: Reward = 23.0\n",
            "Evaluation Episode 6: Reward = 23.0\n",
            "Evaluation Episode 7: Reward = 25.0\n",
            "Evaluation Episode 8: Reward = 19.0\n",
            "Evaluation Episode 9: Reward = 12.0\n",
            "Evaluation Episode 10: Reward = 15.0\n",
            "\n",
            "Mean Evaluation Reward over 10 episodes: 22.7\n"
          ]
        }
      ]
    }
  ]
}