{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Ei7FMLVqT8GdkFHiSF2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import gymnasium as gym\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as distributions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "FdjPZTbIRfDK",
        "outputId": "8e85b42f-7678-4742-ebfc-1a2d0b87d973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'stable_baselines3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1305406866.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import statments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBNaB4Ng0tis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "1e8a8934-ec84-4a73-8b2b-cccbfe51e590"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-6-1752336507.py, line 24)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-6-1752336507.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    super().__init__(*args, **kwargs):\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"LunarLander-v2\" # Changed to v2 for consistency with common practice\n",
        "clipping_coef = 0.1\n",
        "num_envs = 4\n",
        "steps = 1e5\n",
        "middle_layer_size = 64 # Define the size of the middle layer\n",
        "batch_size = 256 # Define a batch size for reshaping\n",
        "\n",
        "# Conventional Vectoriz3ed Environment wrapper\n",
        "def make_env(env_id):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, observation_space_shape, action_space_size, middle_layer_size) -> None:\n",
        "      super().__init__() # Corrected syntax\n",
        "\n",
        "      # Actor/Policy\n",
        "      self.actor = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, action_space_size),\n",
        "          nn.Softmax(dim=-1)\n",
        "          ) # Added dim=-1 to softmax\n",
        "\n",
        "      # Critic/Advantage NN //might need another activation function at the end. Also at somepoint I am going to need to get the values of the S primes which Idk when I am going to do that\n",
        "      self.critic = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, 1), # Output size of 1 for the value function\n",
        "          )\n",
        "\n",
        "  def predict(self, x): # Corrected self parameter\n",
        "    action_probs = self.actor(x)\n",
        "    act_dist = distributions.Categorical(action_probs) # Create a categorical distribution\n",
        "    action = act_dist.sample()\n",
        "\n",
        "    entropy = act_dist.entropy() # Calculate entropy\n",
        "\n",
        "    value_logits = self.critic(x)\n",
        "\n",
        "    # return entropy, probabilies, and sampled action\n",
        "    return (entropy, act_dist, action, value_logits) # Return entropy, probabilities, and a sampled action\n",
        "\n",
        "if __name__ == \"__main__\": # Corrected __main__\n",
        "\n",
        "  # initilize invironment\n",
        "  envs = gym.vector.AsyncVectorEnv([make_env(env_id) for i in range(num_envs)])\n",
        "\n",
        "  # Get observation and action space dimensions\n",
        "  observation_space_shape = envs.single_observation_space.shape[0] # Assuming flat observation space\n",
        "  action_space_size = envs.single_action_space.n # Assuming discrete action space\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Removed args.cuda\n",
        "\n",
        "  #initialize the Agent\n",
        "  agent  = Agent(observation_space_shape, action_space_size, middle_layer_size).to(device) # Pass dimensions and move to device\n",
        "  # put things onto gpu\n",
        "  # ALGO Logic: Storage setup\n",
        "  # These should be lists to store data from each rollout\n",
        "\n",
        "  # Initialize tensors with appropriate shapes\n",
        "  -\n",
        "\n",
        "\n",
        "  # initializes the observation, done, the time, and the step\n",
        "  start_time = time.time()\n",
        "  next_obs = torch.Tensor(envs.reset()[0]).to(device) # Corrected envs.reset()\n",
        "  next_done =  torch.zeros((num_envs,)).to(device)\n",
        "  global_step = 0\n",
        "  step = 0 # Initialize step counter for batch\n",
        "\n",
        "  # define training regime\n",
        "  for i in range(int(steps)): # Cast steps to int\n",
        "\n",
        "    # get actions, observations, rewards, and dones\n",
        "    with torch.no_grad(): # Added no_grad for inference\n",
        "        entropy, action_dist, action, values_ = agent.predict(next_obs) # Renamed values to values_ to avoid conflict\n",
        "\n",
        "    # Store data in tensors at the current step\n",
        "    obs[step] = next_obs\n",
        "    actions[step] = action\n",
        "    logprobs[step] = action_dist.log_prob(action) # Calculate log probability\n",
        "    values[step] = values_.squeeze(-1) # Remove the last dimension of size 1\n",
        "    dones[step] = next_done\n",
        "\n",
        "    next_obs, rewards_, next_done, infos =  envs.step(action.cpu().numpy()) # env step and move action to cpu\n",
        "    rewards[step] = torch.Tensor(rewards_).to(device) # Store rewards as tensor\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.Tensor(next_done).to(device)\n",
        "\n",
        "    global_step += num_envs # Update global step\n",
        "    step += 1 # Increment step counter\n",
        "\n",
        "    # If batch is full, perform training update (This is a placeholder)\n",
        "    if step == batch_size:\n",
        "        # Reshape the collected tensors\n",
        "        b_obs = obs.reshape((-1, observation_space_shape))\n",
        "        b_actions = actions.reshape((-1,)) # Reshape actions to be flat\n",
        "        b_logprobs = logprobs.reshape((-1,))\n",
        "        b_rewards = rewards.reshape((-1,))\n",
        "        b_dones = dones.reshape((-1,))\n",
        "        b_values = values.reshape((-1,))\n",
        "\n",
        "        # This is where your training update logic would go,\n",
        "        # using the reshaped tensors (b_obs, b_actions, etc.)\n",
        "\n",
        "        # Reset step counter and tensors for the next batch\n",
        "        step = 0\n",
        "        obs = torch.zeros((batch_size, num_envs, observation_space_shape)).to(device)\n",
        "        actions = torch.zeros((batch_size, num_envs)).to(device)\n",
        "        logprobs = torch.zeros((batch_size, num_envs)).to(device)\n",
        "        rewards = torch.zeros((batch_size, num_envs)).to(device)\n",
        "        dones = torch.zeros((batch_size, num_envs)).to(device)\n",
        "        values = torch.zeros((batch_size, num_envs)).to(device)\n",
        "\n",
        "\n",
        "    # outer loop: rollouts, at each iteration of the larger loop do one step for your policy and update your value function make sure to clip both the policy and value function. ALso include an entropy term\n",
        "\n",
        "    # inner loop: train value function, step through environment, record rewards, ends, log probs, etc.\n",
        "\n",
        "    # This part of the code needs to be implemented for the training loop (calculating advantages, updating networks, etc.)\n",
        "    # This is a placeholder and would typically involve:\n",
        "    # 1. Calculating advantages/returns\n",
        "    # 2. Calculating policy and value losses\n",
        "    # 3. Performing backpropagation and optimizer steps\n",
        "\n",
        "  envs.close() # Close the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code to get the current time\n",
        "\n",
        "import time\n",
        "\n",
        "print(time.time())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVLiQ78Vjl52",
        "outputId": "225f3d51-9c92-4629-8731-63bb01344b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1752440009.3622656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7026a087"
      },
      "source": [
        "!pip install stable_baselines3 gymnasium"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}