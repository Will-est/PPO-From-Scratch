{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJimeFB/Y9Gxrad12Cs9GI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will-est/PPO-From-Scratch/blob/main/PPO_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statments\n",
        "import gymnasium as gym\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as distributions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "FdjPZTbIRfDK",
        "outputId": "8e85b42f-7678-4742-ebfc-1a2d0b87d973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'stable_baselines3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1305406866.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import statments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBNaB4Ng0tis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "1e8a8934-ec84-4a73-8b2b-cccbfe51e590"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-6-1752336507.py, line 24)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-6-1752336507.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    super().__init__(*args, **kwargs):\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Hper parameters\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "env_id = \"LunarLander-v2\" # Changed to v2 for consistency with common practice\n",
        "clipping_coef = 0.1\n",
        "num_envs = 4\n",
        "rollouts = 1e4\n",
        "middle_layer_size = 64 # Define the size of the middle layer\n",
        "batch_size = 256 # Define a batch size for reshaping\n",
        "\n",
        "# Conventional Vectoriz3ed Environment wrapper\n",
        "def make_env(env_id):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        # Optional: Add wrappers here if needed\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# Initialize info dictionary or maybe a list of dictionaries where each entry contains the mean reward, loss, number of steps, learning rate\n",
        "info = []\n",
        "\n",
        "# Agent definition\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, observation_space_shape, action_space_size, middle_layer_size) -> None:\n",
        "      super().__init__() # Corrected syntax\n",
        "\n",
        "      # Actor/Policy\n",
        "      self.actor = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, action_space_size),\n",
        "          nn.Softmax(dim=-1)\n",
        "          ) # Added dim=-1 to softmax\n",
        "\n",
        "      # Critic/Advantage NN //might need another activation function at the end. Also at somepoint I am going to need to get the values of the S primes which Idk when I am going to do that\n",
        "      self.critic = nn.Sequential(\n",
        "          nn.Linear(observation_space_shape, middle_layer_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(middle_layer_size, 1), # Output size of 1 for the value function\n",
        "          )\n",
        "\n",
        "  def predict(self, x): # Corrected self parameter\n",
        "    action_probs = self.actor(x)\n",
        "    act_dist = distributions.Categorical(action_probs) # Create a categorical distribution\n",
        "    action = act_dist.sample()\n",
        "\n",
        "    entropy = act_dist.entropy() # Calculate entropy\n",
        "\n",
        "    value_logits = self.critic(x)\n",
        "\n",
        "    # return entropy, probabilies, and sampled action\n",
        "    return (entropy, act_dist, action, value_logits) # Return entropy, probabilities, and a sampled action\n",
        "\n",
        "if __name__ == \"__main__\": # Corrected __main__\n",
        "\n",
        "  # initilize invironment\n",
        "  envs = gym.vector.AsyncVectorEnv([make_env(env_id) for i in range(num_envs)])\n",
        "\n",
        "  # Get observation and action space dimensions\n",
        "  observation_space_shape = envs.single_observation_space.shape[0] # Assuming flat observation space\n",
        "  action_space_size = envs.single_action_space.n # Assuming discrete action space\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Removed args.cuda\n",
        "\n",
        "  #initialize the Agent\n",
        "  agent  = Agent(observation_space_shape, action_space_size, middle_layer_size).to(device) # Pass dimensions and move to device\n",
        "  # put things onto gpu\n",
        "  # ALGO Logic: Storage setup\n",
        "  # These should be lists to store data from each rollout\n",
        "\n",
        "  # Initialize tensors with appropriate shapes\n",
        "  step = 0\n",
        "  obs = torch.zeros((batch_size, num_envs, observation_space_shape)).to(device)\n",
        "  actions = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  logprobs = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  rewards = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  dones = torch.zeros((batch_size, num_envs)).to(device)\n",
        "  values = torch.zeros((batch_size, num_envs)).to(device)\n",
        "\n",
        "\n",
        "  # initializes the observation, done, the time, and the step\n",
        "  start_time = time.time()\n",
        "  global_step = 0\n",
        "\n",
        "  # define training regime\n",
        "  for i in range(int(rollouts)): # Cast steps to int\n",
        "    step = 0 # Initialize step counter for batch\n",
        "    next_obs = torch.Tensor(envs.reset()[0]).to(device) # Corrected envs.reset()\n",
        "    next_done =  torch.zeros((num_envs,)).to(device)\n",
        "\n",
        "    for j in range(int(batch_size)):\n",
        "      # get actions, observations, rewards, and dones\n",
        "      with torch.no_grad(): # Added no_grad for inference\n",
        "          entropy, action_dist, action, values_ = agent.predict(next_obs) # Renamed values to values_ to avoid conflict\n",
        "\n",
        "      # Move data to tensors\n",
        "      next_obs_np, rewards_np, next_done_np, infos =  envs.step(action.cpu().numpy()) # env step and move action to cpu\n",
        "      next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "      next_done = torch.Tensor(next_done_np).to(device)\n",
        "\n",
        "      # Store data in tensors at the current step\n",
        "      obs[step] = next_obs\n",
        "      actions[step] = action\n",
        "      logprobs[step] = action_dist.log_prob(action) # Calculate log probability\n",
        "      rewards[step] = torch.Tensor(rewards_np).to(device) # Store rewards as tensor\n",
        "      dones[step] = next_done\n",
        "      values[step] = values_.squeeze(-1) # Remove the last dimension of size 1\n",
        "\n",
        "\n",
        "\n",
        "      global_step += num_envs # Update global step\n",
        "      step += 1 # Increment step counter\n",
        "\n",
        "    # calculate actual values at each time step'\n",
        "\n",
        "    # init actual values and advantages tensor\n",
        "    actual_values = torch.zeros_like(rewards).to(device)\n",
        "    advantages = torch.zeros_like(rewards).to(device)\n",
        "\n",
        "    for t in reversed(range(batch_size)):\n",
        "      if t == batch_size - 1:\n",
        "          # For the last step, if the environment is not done, use the value of the next state (from the agent's prediction)\n",
        "          # Otherwise, the actual value is just the reward at this step\n",
        "          nextnonterminal = 1.0 - next_done\n",
        "          next_value = agent.critic(next_obs).squeeze(-1) // bootstrap next value since it doesn't exsist\n",
        "      else:\n",
        "          # For other steps, if the environment at the next step is not done, use the value of the next state from the stored values\n",
        "          # Otherwise, the actual value is just the reward at this step\n",
        "          nextnonterminal = 1.0 - dones[t+1]\n",
        "          next_value = actual_values[t+1]\n",
        "        actual_values[t] = rewards[t] + gamma * next_value * nextnonterminal\n",
        "    advantages = actual_values - values\n",
        "\n",
        "    # Calculate Values\n",
        "    # inner loop: train value function, step through environment, record rewards, ends, log probs, etc.\n",
        "\n",
        "    # This part of the code needs to be implemented for the training loop (calculating advantages, updating networks, etc.)\n",
        "    # This is a placeholder and would typically involve:\n",
        "    # 1. Calculating advantages/returns\n",
        "    # 2. Calculating policy and value losses\n",
        "    # 3. Performing backpropagation and optimizer steps\n",
        "\n",
        "  envs.close() # Close the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: can you calculate the real value using the function actual_value[t] = reward[t] + gamma*reward[t+1]*is_not_done. Use a reverse forloop, and also use the dones to calculate if an environment has reached an end point or not\n",
        "\n",
        "# Calculate actual values using a reverse for loop\n",
        "actual_values = torch.zeros_like(rewards)\n",
        "lastgaelam = 0\n",
        "gamma = 0.99 # Assuming a gamma value\n",
        "lambda_ = 0.95 # Assuming a lambda value for GAE if needed, though the request only mentions return calculation\n",
        "\n",
        "\n",
        "for t in reversed(range(batch_size)):\n",
        "    if t == batch_size - 1:\n",
        "        # For the last step, if the environment is not done, use the value of the next state (from the agent's prediction)\n",
        "        # Otherwise, the actual value is just the reward at this step\n",
        "        nextnonterminal = 1.0 - next_done\n",
        "        nextvalues = agent.critic(next_obs).squeeze(-1)\n",
        "        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "    else:\n",
        "        # For other steps, if the environment at the next step is not done, use the value of the next state from the stored values\n",
        "        # Otherwise, the actual value is just the reward at this step\n",
        "        nextnonterminal = 1.0 - dones[t+1]\n",
        "        nextvalues = values[t+1]\n",
        "        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zTrH1v8J23e5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}